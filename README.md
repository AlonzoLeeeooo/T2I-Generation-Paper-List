<div align="center">
  <a href="YOUR_OFFICIAL_WEBSITE_URL">
    <img src="assets/logo_run_cn.png" alt="QuenithAI Logo" width="200" height="200">
  </a>
</div>

<div align="center">
  <h1>Awesome Text-to-Image Generation by QuenithAI</h1>
  <p>A curated collection of papers, models, and resources for the field of Text-to-Image Generation.</p>
  <p>
    <a href="https://awesome.re"><img src="https://awesome.re/badge.svg" alt="Awesome"></a>
    &nbsp;
    <a href="https://github.com/QuenithAI/T2I-Generation-Paper-List/pulls"><img src="https://img.shields.io/badge/PRs-Welcome-brightgreen.svg?style=flat-square" alt="PRs Welcome"></a>
    &nbsp;
    <a href="https://github.com/QuenithAI/T2I-Generation-Paper-List/issues"><img src="https://img.shields.io/badge/Issues-Welcome-orange?style=flat-square" alt="Issues Welcome"></a>
  </p>
</div>

> [!NOTE]
> This repository is proudly maintained by the frontline research mentors at **QuenithAI (应达学术)**. It aims to provide the most comprehensive and cutting-edge map of papers and technologies in the field of Text-to-Image generation.
>
> Your contributions are also vital—feel free to [open an issue](https://github.com/QuenithAI/T2I-Generation-Paper-List/issues) or [submit a pull request](https://github.com/QuenithAI/T2I-Generation-Paper-List/pulls) to become a collaborator of this repository. We expect your participation!
> 
>  If you require expert 1-on-1 guidance on your submissions to top-tier conferences and journals, we invite you to **contact us via [WeChat](assets/wechat.jpg) or [E-mail]((mailto:christzhaung@gmail.com))**.
>
>
> ---
>
> 本仓库由 **「应达学术」(QuenithAI)** 的一线科研导师团队倾力打造并持续维护，旨在为您呈现文生图领域最全面、最前沿的论文。
>
> 您的贡献对我们和社区来说至关重要——我们诚邀有志之士通过 [open an issue](https://github.com/QuenithAI/T2I-Generation-Paper-List/issues) 或 [submit a pull request](https://github.com/QuenithAI/T2I-Generation-Paper-List/pulls) 来成为这个项目的合作者之一，期待您的加入！
> 
> 如果您在冲刺科研顶会的道路上需要专业的1V1指导，欢迎**通过[微信](assets/wechat.jpg)或[邮件](mailto:christzhaung@gmail.com)联系我们**。


<details>
<summary><strong>⚡ Latest Updates</strong></summary>

- **(Aug 21th, 2025)**: Add a new direction: [🎨 Personalized Image Generation](#personalized).
- **(Aug 20th, 2025)**: Initial commit and repository structure established.

</details>

---

## <span id="contents">📚 Table of Contents</span>
- [📚 Table of Contents](#-table-of-contents)
- [📜 Papers \& Models](#-papers--models)
  - [✍️ Survey Papers](#️-survey-papers)
  - [🖼️ Text-to-Image Generation](#️-text-to-image-generation)
  - [🕹️ Conditional Image Generation](#️-conditional-image-generation)
  - [🎨 Personalized Image Generation](#-personalized-image-generation)
  - [✂️ Image Editing](#️-image-editing)
- [🗂️ Datasets](#️-datasets)
- [🎓 About Us](#-about-us)
- [🤝 Contributing](#-contributing)

---

## <span id="papers">📜 Papers & Models</span>

### <span id="survey">✍️ Survey Papers</span>



[<small>⇧ Back to ToC</small>](#contents)

### <span id="t2i">🖼️ Text-to-Image Generation</span>

<details>
<summary><h4>✨ 2025</h4></summary>

* **[CVPR 2025]** ***PreciseCam:*** Precise Camera Control for Text-to-Image Generation<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2501.12910)  
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://graphics.unizar.es/projects/PreciseCam2024/)  
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/edurnebernal/PreciseCam)  
    [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/edurnebb/PreciseCam)

* **[CVPR 2025]** ***Type‑R:*** Automatically Retouching Typos for Text‑to‑Image Generation<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2411.18159)  
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/CyberAgentAILab/Type-R)  
    [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/cyberagent/type-r)

* **[CVPR 2025]** ***Compass Control:*** Multi Object Orientation Control for Text‑to‑Image Generation<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2504.06752)  
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://rishubhpar.github.io/CompassControl/)

* **[CVPR 2025]** ***Generative Photography:*** Scene‑Consistent Camera Control for Realistic Text‑to‑Image Synthesis<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.02168)  
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://generative-photography.github.io/project/)  
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/pandayuanyu/generative-photography)  
    [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/pandaphd/generative_photography)

* **[CVPR 2025]** ***One‑Way Ticket:*** Time‑Independent Unified Encoder for Distilling Text‑to‑Image Diffusion Models<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://cvpr.thecvf.com/virtual/2025/poster/32579)  
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/sen-mao/Loopfree)  
    [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/senmaonk/loopfree-sd2.1-base)

* **[CVPR 2025]** ***Text Embedding is Not All You Need:*** Attention Control for Text‑to‑Image Semantic Alignment with Text Self‑Attention Maps<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.15236)  
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://t-sam-diffusion.github.io/)  
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/t-sam-diffusion/code)

* **[CVPR 2025]** ***Towards Uncertainty:*** Understanding and Quantifying Uncertainty for Text‑to‑Image Generation<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.03178)  
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ENSTA-U2IS-AI/Uncertainty_diffusion)

* **[CVPR 2025]** ***Responsible Diffusion:*** Plug‑and‑Play Interpretable Responsible Text‑to‑Image Generation via Dual‑Space Multi‑faceted Concept Control<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.18324)  
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://basim-azam.github.io/responsiblediffusion/)  
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/basim-azam/responsiblediffusion)

* **[CVPR 2025]** ***Make It Count:*** Text‑to‑Image Generation with an Accurate Number of Objects<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2406.10210)  
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://make-it-count-paper.github.io/)  
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Litalby1/make-it-count)

* **[CVPR 2025]** ***MCCD:*** Multi‑Agent Collaboration‑based Compositional Diffusion for Complex Text‑to‑Image Generation<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2505.02648)

* **[CVPR 2025]** ***Debias‑SD:*** Rethinking Training for De‑biasing Text‑to‑Image Generation: Unlocking the Potential of Stable Diffusion<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.12692)

* **[CVPR 2025]** ***ShapeWords:*** Guiding Text‑to‑Image Synthesis with 3D Shape‑Aware Prompts<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.02912)  
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://lodurality.github.io/shapewords/)  
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lodurality/shapewords_paper_code)  
    [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/dmpetrov/shapewords)

* **[CVPR 2025]** ***SnapGen:*** Taming High‑Resolution Text‑to‑Image Models for Mobile Devices with Efficient Architectures and Training<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.09619)  
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://snap-research.github.io/snapgen/)

* **[CVPR 2025]** ***STORM:*** Spatial Transport Optimization by Repositioning Attention Map for Training‑Free Text‑to‑Image Synthesis<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.22168)  
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://micv-yonsei.github.io/storm2025/)  
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/MICV-yonsei/STORM)

* **[CVPR 2025]** ***Focus‑N‑Fix:*** Region‑Aware Fine‑Tuning for Text‑to‑Image Generation<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2501.06481)

* **[CVPR 2025]** ***SILMM:*** Self‑Improving Large Multimodal Models for Compositional Text‑to‑Image Generation<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.05818)  
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://silmm.github.io/)  
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LgQu/SILMM)

* **[CVPR 2025]** ***GLoCE:*** Localized Concept Erasure for Text‑to‑Image Diffusion Models Using Training‑Free Gated Low‑Rank Adaptation<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.12356)  
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://gl-oce.github.io/)  
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Hyun1A/GLoCE)

* **[CVPR 2025]** ***Self‑Cross Guidance:*** Self‑Cross Diffusion Guidance for Text‑to‑Image Synthesis of Similar Subjects<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.18936)  
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://selfcross-guidance.github.io/)  
    [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mengtang-lab/selfcross-guidance)

* **[CVPR 2025]** ***Noise Diffusion:*** Enhancing Semantic Faithfulness in Text‑to‑Image Synthesis<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.16503)  
    [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Bomingmiao/NoiseDiffusion)

* **[CVPR 2025]** ***PromptSampler:*** Learning to Sample Effective and Diverse Prompts for Text‑to‑Image Generation<br>
    [![Paper](https://img.shields.io-badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2410.07838)  
    [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/dbsxodud-11/PAG)

* **[CVPR 2025]** ***STEREO:*** A Two‑Stage Framework for Adversarially Robust Concept Erasing from Text‑to‑Image Diffusion Models<br>
    [![Paper](https://img.shields.io-badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.16807)  
    [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/koushiksrivats/robust-concept-erasing)

* **[CVPR 2025]** ***MinorityPrompt:*** Minority‑Focused Text‑to‑Image Generation via Prompt Optimization<br>
    [![Paper](https://img.shields.io-badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.16503)  
    [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/soobin-um/MinorityPrompt)

* **[CVPR 2025]** ***DistillT5:*** Scaling Down Text Encoders of Text‑to‑Image Diffusion Models<br>
    [![Paper](https://img.shields.io-badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.19897)  
    [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LifuWang-66/DistillT5)

* **[CVPR 2025]** ***TIU:*** The Illusion of Unlearning: The Unstable Nature of Machine Unlearning in Text‑to‑Image Diffusion Models<br>
    [![Paper](https://img.shields.io-badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/??? )  
    [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/NGK2110/TIU)

* **[CVPR 2025]** ***Fuse‑DiT:*** Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text‑to‑Image Synthesis<br>
    [![Paper](https://img.shields.io-badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/??? )  
    [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/tang-bd/fuse-dit)  
    [![Hugging Face](https://img.shields.io-badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/ooutlierr/fuse-dit)

* **[CVPR 2025]** ***Detect‑and‑Guide:*** Self‑regulation of Diffusion Models for Safe Text‑to‑Image Generation via Guideline Token Optimization<br>
    [![Paper](https://img.shields.io-badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.15197)

* **[CVPR 2025]** ***Multi‑Group T2I:*** Multi‑Group Proportional Representations for Text‑to‑Image Models<br>
    [![Paper](https://img.shields.io-badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/papers/Jung_Multi-Group_Proportional_Representations_for_Text-to-Image_Models_CVPR_2025_paper.pdf)

* **[CVPR 2025]** ***VODiff:*** Controlling Object Visibility Order in Text‑to‑Image Generation<br>
    [![Paper](https://img.shields.io-badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/???)  

* **[ICLR 2025]** ***Improving Long‑Text Alignment:*** Improving Long‑Text Alignment for Text‑to‑Image Diffusion Models<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=2ZK8zyIt7o)

* **[ICLR 2025]** ***ITTA:*** Information Theoretic Text‑to‑Image Alignment<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=Ugs2W5XFFo)

* **[ICLR 2025]** ***Meissonic:*** Revitalizing Masked Generative Transformers for Efficient High‑Resolution Text‑to‑Image Synthesis<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=GJsuYHhAga)

* **[ICLR 2025]** ***PaRa:*** Personalizing Text‑to‑Image Diffusion via Parameter Rank Reduction<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=KZgo2YQbhc)

* **[ICLR 2025]** ***Fluid:*** Scaling Autoregressive Text‑to‑image Generative Models with Continuous Tokens<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=jQP5o1VAVc)

* **[ICLR 2025]** ***Prompt‑Pruning:*** Not All Prompts Are Made Equal – Prompt‑based Pruning of Text‑to‑Image Diffusion Models<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=3BhZCfJ73Y)

* **[ICLR 2025]** ***Denoising AR Transformers:*** Denoising Autoregressive Transformers for Scalable Text‑to‑Image Generation<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=amDkNPVWcn)

* **[ICLR 2025]** ***Progressive Compositionality:*** Progressive Compositionality in Text‑to‑Image Generative Models<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=S85PP4xjFD)

* **[ICLR 2025]** ***Classifier Scores:*** Mining your own secrets: Diffusion Classifier Scores for Continual Personalization of Text‑to‑Image Diffusion Models<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=hUdLs6TqZL)

* **[ICLR 2025]** ***Engagement:*** Measuring and Improving Engagement of Text‑to‑Image Generation Models<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=TmCcNuo03f)

* **[ICLR 2025]** ***Residual Gate Eraser:*** Concept Pinpoint Eraser for Text‑to‑image Diffusion Models via Residual Attention Gate<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=ZRDhBwKs7l)

* **[ICLR 2025]** ***Random Seeds:*** Enhancing Compositional Text‑to‑Image Generation with Reliable Random Seeds<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=5BSlakturs)

* **[ICLR 2025]** ***One‑Prompt‑One‑Story:*** Free‑Lunch Consistent Text‑to‑Image Generation Using a Single Prompt<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=cD1kl2QKv1)

* **[ICLR 2025]** ***You Only Sample Once:*** Taming One‑Step Text‑to‑Image Synthesis by Self‑Cooperative Diffusion GANs<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=T7bmHkwzS6)

* **[ICLR 2025]** ***Copyright Revisiting:*** Rethinking Artistic Copyright Infringements in the Era of Text‑to‑Image Generative Models<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=0OTVNEm9N4)

* **[ICLR 2025]** ***Concept Combination Erasing:*** Erasing Concept Combination from Text‑to‑Image Diffusion Model<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=OBjF5I4PWg)

* **[ICLR 2025]** ***Cross‑Attention Patterns:*** Cross‑Attention Head Position Patterns Can Align with Human Visual Concepts in Text‑to‑Image Generative Models<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=1vggIT5vvj)

* **[ICLR 2025]** ***TIGeR:*** Unifying Text‑to‑Image Generation and Retrieval with Large Multimodal Models<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=mr2icR6dpD)

* **[ICLR 2025]** ***DGQ:*** Distribution‑Aware Group Quantization for Text‑to‑Image Diffusion Models<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=ZyNEr7Xw5L)

* **[ICLR 2025]** ***Jacobi Decoding:*** Accelerating Auto‑regressive Text‑to‑Image Generation with Training‑free Speculative Jacobi Decoding<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=LZfjxvqw0N)

* **[ICLR 2025]** ***PT‑T2I/V:*** An Efficient Proxy‑Tokenized Diffusion Transformer for Text‑to‑Image/Video Task<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=lTrrnNdkOX)

* **[ICLR 2025]** ***Gecko Evaluation:*** Revisiting Text‑to‑Image Evaluation with Gecko: on Metrics, Prompts, and Human Rating<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=Im2neAMlre)

* **[ICLR 2025]** ***SANA:*** Efficient High‑Resolution Text‑to‑Image Synthesis with Linear Diffusion Transformers<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=N8Oj1XhtYZ)

* **[ICLR 2025]** ***Rectified Flow:*** Text‑to‑Image Rectified Flow as Plug‑and‑Play Priors<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=SzPZK856iI)

* **[ICLR 2025]** ***Human Feedback Filtering:*** Automated Filtering of Human Feedback Data for Aligning Text‑to‑Image Diffusion Models<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=8jvVNPHtVJ)

* **[ICLR 2025]** ***SAFREE:*** Training‑Free and Adaptive Guard for Safe Text‑to‑Image and Video Generation<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=hgTFotBRKl)

* **[ICLR 2025]** ***IterComp:*** Iterative Composition‑Aware Feedback Learning from Model Gallery for Text‑to‑Image Generation<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=4w99NAikOE)

* **[ICLR 2025]** ***ScImage:*** How good are multimodal large language models at scientific text‑to‑image generation?<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=ugyqNEOjoU)

* **[ICLR 2025]** ***Score Distillation:*** Guided Score Identity Distillation for Data‑Free One‑Step Text‑to‑Image Generation<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=HMVDiaWMwM)

* **[ICLR 2025]** ***Causal Variation:*** Evaluating Semantic Variation in Text‑to‑Image Synthesis: A Causal Perspective<br>
    [![Paper](https://img.shields.io-badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=NWb128pSCb)


<details>
<summary><h4>✅ Published Papers</h4></summary>

</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>✨ 2024</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>✨ 2023</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

[<small>⇧ Back to ToC</small>](#contents)

### <span id="conditional">🕹️ Conditional Image Generation</span>

<details>
<summary><h4>✨ 2025</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>✨ 2024</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>✨ 2023</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

[<small>⇧ Back to ToC</small>](#contents)

### <span id="personalized">🎨 Personalized Image Generation</span>

<details>
<summary><h4>✨ 2025</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>✨ 2024</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>✨ 2023</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

[<small>⇧ Back to ToC</small>](#contents)

### <span id="editing">✂️ Image Editing</span>

<details>
<summary><h4>✨ 2025</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>✨ 2024</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>✨ 2023</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

[<small>⇧ Back to ToC</small>](#contents)



---

## <span id="datasets">🗂️ Datasets</span>
| Dataset Name | Year | Modalities | Task | Paper | Link |
| :--- | :--- | :--- | :--- | :---: | :---: |
| **MS COCO** | 2014 | Text, Image | Text-to-Image Generation, Image Captioning | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/1405.0312) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://cocodataset.org/#home) |
| **Oxford-120 Flowers**| 2008 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://www.robots.ox.ac.uk/~vgg/publications/2008/Nilsback08/nilsback08.pdf) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/) |
| **CUB-200-2011** | 2011 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://resolver.caltech.edu/CaltechCSTR:2010.001) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](http://www.vision.caltech.edu/datasets/cub_200_2011/) |
| **LAION-5B** | 2022 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2210.08402) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://laion.ai/blog/laion-5b/) |
| **DiffusionDB** | 2022 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2210.14896) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://poloclub.github.io/diffusiondb/) |

[<small>⇧ Back to ToC</small>](#contents)

---

## <span id="about-us">🎓 About Us</span>

QuenithAI is a professional organization composed of top researchers, dedicated to providing high-quality 1-on-1 research mentoring for university students worldwide. Our mission is to help students bridge the gap from theoretical knowledge to cutting-edge research and publish their work in top-tier conferences and journals.

Maintaining this `Awesome Text-to-Image Generation` list requires significant effort, just as completing a high-quality paper requires focused dedication and expert guidance. If you're looking for one-on-one support from top scholars on your own research project, to quickly identify innovative ideas and make publications, we invite you to contact us ASAP.

➡️ **Contact us via [WeChat](assets/wechat.jpg) or [E-mail](mailto:your.email@example.com) to start your research journey.**

---

「应达学术」(QuenithAI) 是一家由顶尖研究者组成，致力于为全球高校学生提供高质量1V1科研辅导的专业机构。我们的使命是帮助学生培养出色卓越的科研技能，在顶级会议和期刊上发表自己的成果。

维护一个GitHub调研仓库需要巨大的精力，正如完成一篇高质量的论文一样，离不开专注的投入和专业的指导。如果您希望在自己的研究项目中，获得来自顶尖学者的一对一支持，我们诚邀您与我们取得联系。

➡️ **欢迎通过 [微信](assets/wechat.jpg) 或 [邮件](mailto:your.email@example.com) 联系我们，开启您的科研之旅。**


[<small>⇧ Back to ToC</small>](#contents)

---



## <span id="contributing">🤝 Contributing</span>

Contributions are welcome! Please see our [**Contribution Guidelines**](CONTRIBUTING.md) for details on how to add new papers, correct information, or improve the repository.