<div align="center">
  <a href="YOUR_OFFICIAL_WEBSITE_URL">
    <img src="assets/logo_run_cn.png" alt="QuenithAI Logo" width="200" height="200">
  </a>
</div>

<div align="center">
  <h1>Awesome Text-to-Image Generation by QuenithAI</h1>
  <p>A curated collection of papers, models, and resources for the field of Text-to-Image Generation.</p>
  <p>
    <a href="https://awesome.re"><img src="https://awesome.re/badge.svg" alt="Awesome"></a>
    &nbsp;
    <a href="https://github.com/QuenithAI/T2I-Generation-Paper-List/pulls"><img src="https://img.shields.io/badge/PRs-Welcome-brightgreen.svg?style=flat-square" alt="PRs Welcome"></a>
    &nbsp;
    <a href="https://github.com/QuenithAI/T2I-Generation-Paper-List/issues"><img src="https://img.shields.io/badge/Issues-Welcome-orange?style=flat-square" alt="Issues Welcome"></a>
  </p>
</div>

> [!NOTE]
> This repository is proudly maintained by the frontline research mentors at **QuenithAI (应达学术)**. It aims to provide the most comprehensive and cutting-edge map of papers and technologies in the field of Text-to-Image generation.
>
> Your contributions are also vital—feel free to [open an issue](https://github.com/QuenithAI/T2I-Generation-Paper-List/issues) or [submit a pull request](https://github.com/QuenithAI/T2I-Generation-Paper-List/pulls) to become a collaborator of this repository. We expect your participation!
> 
>  If you require expert 1-on-1 guidance on your submissions to top-tier conferences and journals, we invite you to **contact us via [WeChat](assets/wechat.jpg) or [E-mail]((mailto:christzhaung@gmail.com))**.
>
>
> ---
>
> 本仓库由 **「应达学术」(QuenithAI)** 的一线科研导师团队倾力打造并持续维护，旨在为您呈现文生图领域最全面、最前沿的论文。
>
> 您的贡献对我们和社区来说至关重要——我们诚邀有志之士通过 [open an issue](https://github.com/QuenithAI/T2I-Generation-Paper-List/issues) 或 [submit a pull request](https://github.com/QuenithAI/T2I-Generation-Paper-List/pulls) 来成为这个项目的合作者之一，期待您的加入！
> 
> 如果您在冲刺科研顶会的道路上需要专业的1V1指导，欢迎**通过[微信](assets/wechat.jpg)或[邮件](mailto:christzhaung@gmail.com)联系我们**。


<details>
<summary><strong>⚡ Latest Updates</strong></summary>

- **(Aug 21th, 2025)**: Add a new direction: [🎨 Personalized Image Generation](#personalized).
- **(Aug 20th, 2025)**: Initial commit and repository structure established.

</details>

---

## <span id="contents">📚 Table of Contents</span>
- [📚 Table of Contents](#-table-of-contents)
- [📜 Papers \& Models](#-papers--models)
  - [✍️ Survey Papers](#️-survey-papers)
  - [🖼️ Text-to-Image Generation](#️-text-to-image-generation)
  - [🕹️ Conditional Image Generation](#️-conditional-image-generation)
  - [🎨 Personalized Image Generation](#-personalized-image-generation)
  - [✂️ Image Editing](#️-image-editing)
- [🗂️ Datasets](#️-datasets)
- [🎓 About Us](#-about-us)
- [🤝 Contributing](#-contributing)

---

## <span id="papers">📜 Papers & Models</span>

### <span id="survey">✍️ Survey Papers</span>



[<small>⇧ Back to ToC</small>](#contents)

### <span id="t2i">🖼️ Text-to-Image Generation</span>

<details>
<summary><h4>✨ 2025</h4></summary>


<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2025]** ***PreciseCam:*** *Precise Camera Control for Text-to-Image Generation*<br>
   [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2501.12910)
  [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://graphics.unizar.es/projects/PreciseCam2024/)
  [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/edurnebernal/PreciseCam)
  [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/edurnebb/PreciseCam)

* **[CVPR 2025]** ***Type‑R:*** *Automatically Retouching Typos for Text‑to‑Image Generation*<br>
   [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2411.18159) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/CyberAgentAILab/Type-R) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/cyberagent/type-r)

* **[CVPR 2025]** ***Compass Control:*** *Multi Object Orientation Control for Text‑to‑Image Generation*<br>
   [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2504.06752) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://rishubhpar.github.io/CompassControl/)

* **[CVPR 2025]** ***Generative Photography:*** *Scene‑Consistent Camera Control for Realistic Text‑to‑Image Synthesis*<br>
   [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.02168) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://generative-photography.github.io/project/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/pandayuanyu/generative-photography) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/pandaphd/generative_photography)

* **[CVPR 2025]** ***One‑Way Ticket:*** *Time‑Independent Unified Encoder for Distilling Text‑to‑Image Diffusion Models*<br>
   [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://cvpr.thecvf.com/virtual/2025/poster/32579) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/sen-mao/Loopfree) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/senmaonk/loopfree-sd2.1-base)

* **[CVPR 2025]** ***Text Embedding is Not All You Need:*** *Attention Control for Text‑to‑Image Semantic Alignment with Text Self‑Attention Maps*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.15236) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://t-sam-diffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/t-sam-diffusion/code)

* **[CVPR 2025]** ***Towards Uncertainty:*** *Understanding and Quantifying Uncertainty for Text‑to‑Image Generation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.03178) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ENSTA-U2IS-AI/Uncertainty_diffusion)

* **[CVPR 2025]** ***Responsible Diffusion:*** *Plug‑and‑Play Interpretable Responsible Text‑to‑Image Generation via Dual‑Space Multi‑faceted Concept Control*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.18324) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://basim-azam.github.io/responsiblediffusion/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/basim-azam/responsiblediffusion)

* **[CVPR 2025]** ***Make It Count:*** *Text‑to‑Image Generation with an Accurate Number of Objects*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2406.10210) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://make-it-count-paper.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Litalby1/make-it-count)

* **[CVPR 2025]** ***MCCD:*** *Multi‑Agent Collaboration‑based Compositional Diffusion for Complex Text‑to‑Image Generation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2505.02648)

* **[CVPR 2025]** ***Debias‑SD:*** *Rethinking Training for De‑biasing Text‑to‑Image Generation: Unlocking the Potential of Stable Diffusion*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.12692)

* **[CVPR 2025]** ***ShapeWords:*** *Guiding Text‑to‑Image Synthesis with 3D Shape‑Aware Prompts*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.02912) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://lodurality.github.io/shapewords/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lodurality/shapewords_paper_code) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/dmpetrov/shapewords)

* **[CVPR 2025]** ***SnapGen:*** *Taming High‑Resolution Text‑to‑Image Models for Mobile Devices with Efficient Architectures and Training*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.09619) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://snap-research.github.io/snapgen/)

* **[CVPR 2025]** ***STORM:*** *Spatial Transport Optimization by Repositioning Attention Map for Training‑Free Text‑to‑Image Synthesis*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.22168) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://micv-yonsei.github.io/storm2025/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/MICV-yonsei/STORM)

* **[CVPR 2025]** ***Focus‑N‑Fix:*** *Region‑Aware Fine‑Tuning for Text‑to‑Image Generation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2501.06481)

* **[CVPR 2025]** ***SILMM:*** *Self‑Improving Large Multimodal Models for Compositional Text‑to‑Image Generation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.05818) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://silmm.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LgQu/SILMM)

* **[CVPR 2025]** ***GLoCE:*** *Localized Concept Erasure for Text‑to‑Image Diffusion Models Using Training‑Free Gated Low‑Rank Adaptation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.12356) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://gl-oce.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Hyun1A/GLoCE)

* **[CVPR 2025]** ***Self‑Cross Guidance:*** *Self‑Cross Diffusion Guidance for Text‑to‑Image Synthesis of Similar Subjects*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.18936) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://selfcross-guidance.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mengtang-lab/selfcross-guidance)

* **[CVPR 2025]** ***Noise Diffusion:*** *Enhancing Semantic Faithfulness in Text‑to‑Image Synthesis*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.16503) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Bomingmiao/NoiseDiffusion)

* **[CVPR 2025]** ***PromptSampler:*** *Learning to Sample Effective and Diverse Prompts for Text‑to‑Image Generation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2410.07838) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/dbsxodud-11/PAG)

* **[CVPR 2025]** ***STEREO:*** *A Two‑Stage Framework for Adversarially Robust Concept Erasing from Text‑to‑Image Diffusion Models*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.16807) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/koushiksrivats/robust-concept-erasing)

* **[CVPR 2025]** ***MinorityPrompt:*** *Minority‑Focused Text‑to‑Image Generation via Prompt Optimization*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.16503) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/soobin-um/MinorityPrompt)

* **[CVPR 2025]** ***DistillT5:*** *Scaling Down Text Encoders of Text‑to‑Image Diffusion Models*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.19897) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LifuWang-66/DistillT5)

* **[CVPR 2025]** ***TIU:*** *The Illusion of Unlearning: The Unstable Nature of Machine Unlearning in Text‑to‑Image Diffusion Models*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/???) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/NGK2110/TIU)

* **[CVPR 2025]** ***Fuse‑DiT:*** *Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text‑to‑Image Synthesis*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/???) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/tang-bd/fuse-dit) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/ooutlierr/fuse-dit)

* **[CVPR 2025]** **Detect‑and‑Guide:** *Self‑regulation of Diffusion Models for Safe Text‑to‑Image Generation via Guideline Token Optimization*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.15197)

* **[CVPR 2025]** **Multi‑Group T2I:** *Multi‑Group Proportional Representations for Text‑to‑Image Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/papers/Jung_Multi-Group_Proportional_Representations_for_Text-to-Image_Models_CVPR_2025_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/sangwon-jung94/mpr-t2i)

* **[CVPR 2025]** **VODiff:** *Controlling Object Visibility Order in Text‑to‑Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/???) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/dliang293/VODiff)

* **[CVPR 2025]** *Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Shin_Large-Scale_Text-to-Image_Model_with_Inpainting_is_a_Zero-Shot_Subject-Driven_Image_CVPR_2025_paper.html) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://diptychprompting.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/chaehunshin/DiptychPrompting) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta)

* **[CVPR 2025]** *Six‑CD: Benchmarking Concept Removals for Text-to-image Diffusion Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Ren_Six-CD_Benchmarking_Concept_Removals_for_Text-to-image_Diffusion_Models_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Artanisax/Six-CD)

* **[CVPR 2025]** *ConceptGuard: Continual Personalized Text-to-Image Generation with Forgetting and Confusion Mitigation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Guo_ConceptGuard_Continual_Personalized_Text-to-Image_Generation_with_Forgetting_and_Confusion_Mitigation_CVPR_2025_paper.html)

* **[CVPR 2025]** *ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Jia_ChatGen_Automatic_Text-to-Image_Generation_From_FreeStyle_Chatting_CVPR_2025_paper.html) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://chengyou-jia.github.io/ChatGen-Home/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/chengyou-jia/ChatGen) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/ChengyouJia/ChatGen-Base-8B)

* **[ICLR 2025]** **Improving Long‑Text Alignment:** *Improving Long‑Text Alignment for Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=2ZK8zyIt7o) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/luping-liu/LongAlign)

* **[ICLR 2025]** **ITTA:** *Information Theoretic Text‑to‑Image Alignment*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=Ugs2W5XFFo) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Chao0511/mitune)

* **[ICLR 2025]** **Meissonic:** *Revitalizing Masked Generative Transformers for Efficient High‑Resolution Text‑to‑Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=GJsuYHhAga) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sites.google.com/view/meissonic/home) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/viiika/Meissonic) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/MeissonFlow/Meissonic)

* **[ICLR 2025]** **PaRa:** *Personalizing Text‑to‑Image Diffusion via Parameter Rank Reduction*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=KZgo2YQbhc)

* **[ICLR 2025]** **Fluid:** *Scaling Autoregressive Text‑to‑image Generative Models with Continuous Tokens*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=jQP5o1VAVc)

* **[ICLR 2025]** **Prompt‑Pruning:** *Not All Prompts Are Made Equal – Prompt‑based Pruning of Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=3BhZCfJ73Y) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/rezashkv/diffusion_pruning) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/rezashkv/diffusion_pruning)

* **[ICLR 2025]** **Denoising AR Transformers:** *Denoising Autoregressive Transformers for Scalable Text‑to‑Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=amDkNPVWcn)

* **[ICLR 2025]** **Progressive Compositionality:** *Progressive Compositionality in Text‑to‑Image Generative Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=S85PP4xjFD) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://evansh666.github.io/EvoGen_Page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/evansh666/EvoGen)

* **[ICLR 2025]** **Classifier Scores:** *Mining your own secrets: Diffusion Classifier Scores for Continual Personalization of Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=hUdLs6TqZL) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://srvcodes.github.io/continual_personalization)

* **[ICLR 2025]** **Engagement:** *Measuring and Improving Engagement of Text‑to‑Image Generation Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=TmCcNuo03f) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://behavior-in-the-wild.github.io/image-engagement)

* **[ICLR 2025]** **Residual Gate Eraser:** *Concept Pinpoint Eraser for Text‑to-image Diffusion Models via Residual Attention Gate*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=ZRDhBwKs7l) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Hyun1A/CPE)

* **[ICLR 2025]** **Random Seeds:** *Enhancing Compositional Text‑to‑Image Generation with Reliable Random Seeds*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=5BSlakturs) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/doub7e/Reliable-Random-Seeds)

* **[ICLR 2025]** **One‑Prompt‑One‑Story:** *Free‑Lunch Consistent Text‑to‑Image Generation Using a Single Prompt*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=cD1kl2QKv1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://byliutao.github.io/1Prompt1Story.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/byliutao/1Prompt1Story)

* **[ICLR 2025]** **You Only Sample Once:** *Taming One‑Step Text‑to‑Image Synthesis by Self‑Cooperative Diffusion GANs*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=T7bmHkwzS6) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yoso-t2i.github.io) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Luo-Yihong/YOSO) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/Luo-Yihong/yoso_pixart512)

* **[ICLR 2025]** **Copyright Revisiting:** *Rethinking Artistic Copyright Infringements in the Era of Text‑to‑Image Generative Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=0OTVNEm9N4)

* **[ICLR 2025]** **Concept Combination Erasing:** *Erasing Concept Combination from Text‑to‑Image Diffusion Model*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=OBjF5I4PWg)

* **[ICLR 2025]** **Cross‑Attention Patterns:** *Cross‑Attention Head Position Patterns Can Align with Human Visual Concepts in Text‑to‑Image Generative Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=1vggIT5vvj) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/SNU-DRL/HRV)

* **[ICLR 2025]** **TIGeR:** *Unifying Text‑to‑Image Generation and Retrieval with Large Multimodal Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=mr2icR6dpD) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://tiger-t2i.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LgQu/TIGeR)

* **[ICLR 2025]** **DGQ:** *Distribution‑Aware Group Quantization for Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=ZyNEr7Xw5L) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ugonfor/DGQ)

* **[ICLR 2025]** **Jacobi Decoding:** *Accelerating Auto‑regressive Text‑to‑Image Generation with Training‑free Speculative Jacobi Decoding*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=LZfjxvqw0N) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/tyshiwo1/Accelerating-T2I-AR-with-SJD)

* **[ICLR 2025]** **PT‑T2I/V:** *An Efficient Proxy‑Tokenized Diffusion Transformer for Text‑to‑Image/Video Task*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=lTrrnNdkOX) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://360cvgroup.github.io/Qihoo-T2X/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/360CVGroup/Qihoo-T2X)

* **[ICLR 2025]** **Gecko Evaluation:** *Revisiting Text‑to‑Image Evaluation with Gecko: on Metrics, Prompts, and Human Rating*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=Im2neAMlre) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/google-deepmind/gecko_benchmark_t2i)

* **[ICLR 2025]** **SANA:** *Efficient High‑Resolution Text‑to‑Image Synthesis with Linear Diffusion Transformers*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=N8Oj1XhtYZ) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://nvlabs.github.io/Sana) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/NVlabs/Sana) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/Efficient-Large-Model/Sana_1600M_1024px_diffusers)

* **[ICLR 2025]** **Rectified Flow:** *Text‑to‑Image Rectified Flow as Plug‑and‑Play Priors*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=SzPZK856iI) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/yangxiaofeng/rectified_flow_prior)

* **[ICLR 2025]** **Human Feedback Filtering:** *Automated Filtering of Human Feedback Data for Aligning Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=8jvVNPHtVJ)

* **[ICLR 2025]** **SAFREE:** *Training‑Free and Adaptive Guard for Safe Text‑to‑Image and Video Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=hgTFotBRKl) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://safree-safe-t2i-t2v.github.io) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/jaehong31/SAFREE)

* **[ICLR 2025]** **IterComp:** *Iterative Composition‑Aware Feedback Learning from Model Gallery for Text‑to‑Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=4w99NAikOE) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/YangLing0818/IterComp) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/comin/IterComp)

* **[ICLR 2025]** **ScImage:** *How good are multimodal large language models at scientific text‑to‑image generation?*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=ugyqNEOjoU) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/casszhao/ScImage)

* **[ICLR 2025]** **Score Distillation:** *Guided Score Identity Distillation for Data‑Free One‑Step Text‑to‑Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=HMVDiaWMwM) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mingyuanzhou/SiD-LSG) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/UT-Austin-PML/SiD-LSG)

* **[ICLR 2025]** **Causal Variation:** *Evaluating Semantic Variation in Text‑to‑Image Synthesis: A Causal Perspective*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=NWb128pSCb) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zhuxiangru/SemVarBench)

</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>


- [Plot'n Polish: Zero‑shot Story Visualization and Disentangled Editing with Text‑to‑Image Diffusion Models](http://arxiv.org/abs/2509.04446v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://plotnpolish.github.io)
- [Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model](http://arxiv.org/abs/2509.04548v1) [![GitHub Stars](https://img.shields.io/github/stars/SkyworkAI/UniPic?style=social)](https://github.com/SkyworkAI/UniPic) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://unipic-v2.github.io/)
- [PromptEnhancer: A Simple Approach to Enhance Text‑to‑Image Models via Chain‑of‑Thought Prompt Rewriting](http://arxiv.org/abs/2509.04545v1)
- [From Editor to Dense Geometry Estimator](http://arxiv.org/abs/2509.04338v1)
- [Noisy Label Refinement with Semantically Reliable Synthetic Images](http://arxiv.org/abs/2509.04298v1)
- [MEPG:Multi‑Expert Planning and Generation for Compositionally‑Rich Image Generation](http://arxiv.org/abs/2509.04126v1)
- [Easier Painting Than Thinking: Can Text‑to‑Image Models Set the Stage, but Not Direct the Play?](http://arxiv.org/abs/2509.03516v1)
- [Fidelity‑preserving enhancement of ptychography with foundational text‑to‑image models](http://arxiv.org/abs/2509.04513v1)
- [Exploring Diffusion Models for Generative Forecasting of Financial Charts](http://arxiv.org/abs/2509.02308v1)
- [Data‑Driven Loss Functions for Inference‑Time Optimization in Text‑to‑Image Generation](http://arxiv.org/abs/2509.02295v1)
- [Palette Aligned Image Diffusion](http://arxiv.org/abs/2509.02000v1)
- [Draw‑In‑Mind: Learning Precise Image Editing via Chain‑of‑Thought Imagination](http://arxiv.org/abs/2509.01986v1) [![GitHub Stars](https://img.shields.io/github/stars/showlab/DIM?style=social)](https://github.com/showlab/DIM)
- [Discrete Noise Inversion for Next‑scale Autoregressive Text‑based Image Editing](http://arxiv.org/abs/2509.01984v2)
- [Q‑Sched: Pushing the Boundaries of Few‑Step Diffusion Models with Quantization‑Aware Scheduling](http://arxiv.org/abs/2509.01624v1)
- [RealMat: Realistic Materials with Diffusion and Reinforcement Learning](http://arxiv.org/abs/2509.01134v1)
- [CompSlider: Compositional Slider for Disentangled Multiple‑Attribute Image Generation](http://arxiv.org/abs/2509.01028v2)
- [Prompting Away Stereotypes? Evaluating Bias in Text‑to‑Image Models for Occupations](http://arxiv.org/abs/2509.00849v1)
- [Multi‑Level CLS Token Fusion for Contrastive Learning in Endoscopy Image Classification](http://arxiv.org/abs/2509.00752v1)
- [HADIS: Hybrid Adaptive Diffusion Model Serving for Efficient Text‑to‑Image Generation](http://arxiv.org/abs/2509.00642v1)
- [AMCR: A Framework for Assessing and Mitigating Copyright Risks in Generative Models](http://arxiv.org/abs/2509.00641v1)
- [Reusing Computation in Text‑to‑Image Diffusion for Efficient Generation of Image Sets](http://arxiv.org/abs/2508.21032v1)
- [Understanding and evaluating computer vision models through the lens of counterfactuals](http://arxiv.org/abs/2508.20881v1)
- [Pref‑GRPO: Pairwise Preference Reward‑based GRPO for Stable Text‑to‑Image Reinforcement Learning](http://arxiv.org/abs/2508.20751v1)
- [Persode: Personalized Visual Journaling with Episodic Memory‑Aware AI Agent](http://arxiv.org/abs/2508.20585v1)
- [Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent](http://arxiv.org/abs/2508.20505v1)
- [Safe‑Control: A Safety Patch for Mitigating Unsafe Content in Text‑to‑Image Generation Models](http://arxiv.org/abs/2508.21099v1)
- [Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color Perception in Text‑to‑Image Models](http://arxiv.org/abs/2508.19791v1)
- [MonoRelief V2: Leveraging Real Data for High‑Fidelity Monocular Relief Recovery](http://arxiv.org/abs/2508.19555v1)
- [All‑in‑One Slider for Attribute Manipulation in Diffusion Models](http://arxiv.org/abs/2508.19195v1) 
- [Visual‑CoG: Stage‑Aware Reinforcement Learning with Chain of Guidance for Text‑to‑Image Generation](http://arxiv.org/abs/2508.18032v2)
- [CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text‑to‑Image Generation](http://arxiv.org/abs/2508.17760v1)
- [Instant Preference Alignment for Text‑to‑Image Diffusion Models](http://arxiv.org/abs/2508.17718v1)
- [T2I‑ReasonBench: Benchmarking Reasoning‑Informed Text‑to‑Image Generation](http://arxiv.org/abs/2508.17472v1)
- [Bias Amplification in Stable Diffusion's Representation of Stigma Through Skin Tones and Their Homogeneity](http://arxiv.org/abs/2508.17465v1)
- [An LLM‑LVLM Driven Agent for Iterative and Fine‑Grained Image Editing](http://arxiv.org/abs/2508.17435v1)
- [HiCache: Training‑free Acceleration of Diffusion Models via Hermite Polynomial‑based Feature Caching](http://arxiv.org/abs/2508.16984v1)
- [Delta‑SVD: Efficient Compression for Personalized Text‑to‑Image Models](http://arxiv.org/abs/2508.16863v1)
- [Improving Performance, Robustness, and Fairness of Radiographic AI Models with Finely‑Controllable Synthetic Data](http://arxiv.org/abs/2508.16783v1)
- [A Framework for Benchmarking Fairness‑Utility Trade‑offs in Text‑to‑Image Models via Pareto Frontiers](http://arxiv.org/abs/2508.16752v1)
- [A‑FloPS: Accelerating Diffusion Sampling with Adaptive Flow Path Sampler](http://arxiv.org/abs/2509.00036v1)
- [UniEM‑3M: A Universal Electron Micrograph Dataset for Microstructural Segmentation and Generation](http://arxiv.org/abs/2508.16239v1)
- [RAGSR: Regional Attention Guided Diffusion for Image Super‑Resolution](http://arxiv.org/abs/2508.16158v1)
- [Scaling Group Inference for Diverse and High‑Quality Generation](http://arxiv.org/abs/2508.15773v1)
- [Waver: Wave Your Way to Lifelike Video Generation](http://arxiv.org/abs/2508.15761v2)
- [GenTune: Toward Traceable Prompts to Improve Controllability of Image Refinement in Environment Design](http://arxiv.org/abs/2508.15227v1)
- [Side Effects of Erasing Concepts from Diffusion Models](http://arxiv.org/abs/2508.15124v2)
- [CurveFlow: Curvature‑Guided Flow Matching for Image Generation](http://arxiv.org/abs/2508.15093v2)
- [SATURN: Autoregressive Image Generation Guided by Scene Graphs](http://arxiv.org/abs/2508.14502v1)
- [MUSE: Multi‑Subject Unified Synthesis via Explicit Layout Semantic Expansion](http://arxiv.org/abs/2508.14440v1)
- [CTA‑Flux: Integrating Chinese Cultural Semantics into High‑Quality English Text‑to‑Image Communities](http://arxiv.org/abs/2508.14405v1)
- [Sealing The Backdoor: Unlearning Adversarial Text Triggers In Diffusion Models Using Knowledge Distillation](http://arxiv.org/abs/2508.18235v1)
- [Inference Time Debiasing Concepts in Diffusion Models](http://arxiv.org/abs/2508.14933v1)
- [Pixels Under Pressure: Exploring Fine‑Tuning Paradigms for Foundation Models in High‑Resolution Medical Imaging](http://arxiv.org/abs/2508.14931v1)
- [SAGA: Learning Signal‑Aligned Distributions for Improved Text‑to‑Image Generation](http://arxiv.org/abs/2508.13866v1)
- [UniECS: Unified Multimodal E‑Commerce Search Framework with Gated Cross‑modal Fusion](http://arxiv.org/abs/2508.13843v1)
- [DiffIER: Optimizing Diffusion Models with Iterative Error Reduction](http://arxiv.org/abs/2508.13628v2)
- [7Bench: a Comprehensive Benchmark for Layout‑guided Text‑to‑image Models](http://arxiv.org/abs/2508.12919v1)
- [S²‑Guidance: Stochastic Self Guidance for Training‑Free Enhancement of Diffusion Models](http://arxiv.org/abs/2508.12880v1)
- [Single‑Reference Text‑to‑Image Manipulation with Dual Contrastive Denoising Score](http://arxiv.org/abs/2508.12718v1)
- [DeCoT: Decomposing Complex Instructions for Enhanced Text‑to‑Image Generation with Large Language Models](http://arxiv.org/abs/2508.12396v1)
- [Navigating the Exploration‑Exploitation Tradeoff in Inference‑Time Scaling of Diffusion Models](http://arxiv.org/abs/2508.12361v1)
- [SafeCtrl: Region‑Based Safety Control for Text‑to‑Image Diffusion via Detect‑Then‑Suppress](http://arxiv.org/abs/2508.11904v1)
- [LoRAtorio: An intrinsic approach to LoRA Skill Composition](http://arxiv.org/abs/2508.11624v1)
- [SPG: Style‑Prompting Guidance for Style‑Specific Content Creation](http://arxiv.org/abs/2508.11476v1)
- [Match & Choose: Model Selection Framework for Fine‑tuning Text‑to‑Image Diffusion Models](http://arxiv.org/abs/2508.10993v1)
- [NextStep‑1: Toward Autoregressive Image Generation with Continuous Tokens at Scale](http://arxiv.org/abs/2508.10711v2)
- [CountCluster: Training‑Free Object Quantity Guidance with Cross‑Attention Map Clustering for Text‑to‑Image Generation](http://arxiv.org/abs/2508.10710v1)
- [NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer](http://arxiv.org/abs/2508.10424v1)
- [Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text‑to‑Image Diffusion Models](http://arxiv.org/abs/2508.10407v2)
- [High Fidelity Text to Image Generation with Contrastive Alignment and Structural Guidance](http://arxiv.org/abs/2508.10280v1)
- [Echo‑4o: Harnessing the Power of GPT‑4o Synthetic Images for Improved Image Generation](http://arxiv.org/abs/2508.09987v1)
- [WeDesign: Generative AI‑Facilitated Community Consultations for Urban Public Space Design](http://arxiv.org/abs/2508.19256v1)
- [Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality](http://arxiv.org/abs/2508.09598v1)
- [Dual Recursive Feedback on Generation and Appearance Latents for Pose‑Robust Text‑to‑Image Diffusion](http://arxiv.org/abs/2508.09575v1)
- [Understanding Dementia Speech Alignment with Diffusion‑Based Image Generation](http://arxiv.org/abs/2508.09385v1)
- [Per‑Query Visual Concept Learning](http://arxiv.org/abs/2508.09045v1)
- [TARA: Token‑Aware LoRA for Composable Personalization in Diffusion Models](http://arxiv.org/abs/2508.08812v1)
- [Exploring Palette based Color Guidance in Diffusion Models](http://arxiv.org/abs/2508.08754v1)
- [SafeFix: Targeted Model Repair via Controlled Image Generation](http://arxiv.org/abs/2508.08701v1)
- [CLUE: Leveraging Low‑Rank Adaptation to Capture Latent Uncovered Evidence for Image Forgery Localization](http://arxiv.org/abs/2508.07413v1)
- [CoAR: Concept Injection into Autoregressive Models for Personalized Text‑to‑Image Generation](http://arxiv.org/abs/2508.07341v1)
- [Multi‑task Adversarial Attacks against Black‑box Model with Few‑shot Queries](http://arxiv.org/abs/2508.10039v1)
- [Explainability‑in‑Action: Enabling Expressive Manipulation and Tacit Understanding by Bending Diffusion Models in ComfyUI](http://arxiv.org/abs/2508.07183v1)
- [Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities](http://arxiv.org/abs/2508.07031v1)
- [HiMat: DiT‑based Ultra‑High Resolution SVBRDF Generation](http://arxiv.org/abs/2508.07011v2)
- [CannyEdit: Selective Canny Control and Dual‑Prompt Guidance for Training‑Free Image Editing](http://arxiv.org/abs/2508.06937v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://vaynexie.github.io/CannyEdit)
- [AR‑GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning](http://arxiv.org/abs/2508.06924v1)
- [Talk2Image: A Multi‑Agent System for Multi‑Turn Image Generation and Editing](http://arxiv.org/abs/2508.06916v1)
- [Towards Effective Prompt Stealing Attack against Text‑to‑Image Diffusion Models](http://arxiv.org/abs/2508.06837v1)
- [Restage4D: Reanimating Deformable 3D Reconstruction from a Single Video](http://arxiv.org/abs/2508.06715v1)
- [VISTAR:A User‑Centric and Role‑Driven Benchmark for Text‑to‑Image Evaluation](http://arxiv.org/abs/2508.06152v1)
- [NEP: Autoregressive Image Editing via Next Editing Token Prediction](http://arxiv.org/abs/2508.06044v1)
- [Learning 3D Texture‑Aware Representations for Parsing Diverse Human Clothing and Body Parts](http://arxiv.org/abs/2508.06032v1)
- [UnGuide: Learning to Forget with LoRA‑Guided Diffusion Models](http://arxiv.org/abs/2508.05755v1)
- [Whose Truth? Pluralistic Geo‑Alignment for (Agentic) AI](http://arxiv.org/abs/2508.05432v1)
- [UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text‑to‑Image Generation](http://arxiv.org/abs/2508.05399v1)
- [Textual Inversion for Efficient Adaptation of Open‑Vocabulary Object Detectors Without Forgetting](http://arxiv.org/abs/2508.05323v1)
- [ACM Multimedia Grand Challenge on ENT Endoscopy Analysis](http://arxiv.org/abs/2508.04801v1)



</details>

</details>

<details>
<summary><h4>✨ 2024</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2024]** ***DistriFusion:*** *Distributed Parallel Inference for High-Resolution Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.19481.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mit-han-lab/distrifuser)

* **[CVPR 2024]** ***InstanceDiffusion:*** *Instance-level Control for Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.03290.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://people.eecs.berkeley.edu/~xdwang/projects/InstDiff/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/frank-xwang/InstanceDiffusion)

* **[CVPR 2024]** ***ECLIPSE:*** *A Resource-Efficient Text-to-Image Prior for Image Generations*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.04655.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://github.com/eclipse-t2i/eclipse-inference) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://eclipse-t2i.vercel.app/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/ECLIPSE-Community/ECLIPSE-Kandinsky-v2.2)

* **[CVPR 2024]** ***Instruct-Imagen:*** *Image Generation with Multi-modal Instruction*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2401.01952.pdf)

* **[CVPR 2024]** ***Continuous 3D Words:*** *Learning Continuous 3D Words for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.08654.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ttchengab/continuous_3d_words_code/)

* **[CVPR 2024]** ***HanDiffuser:*** *Text-to-Image Generation With Realistic Hand Appearances*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.01693.pdf)

* **[CVPR 2024]** ***Rich Human Feedback:*** *Rich Human Feedback for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.10240.pdf)

* **[CVPR 2024]** ***MarkovGen:*** *Structured Prediction for Efficient Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2308.10997.pdf)

* **[CVPR 2024]** ***Customization Assistant:*** *Customization Assistant for Text-to-image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.03045.pdf)

* **[CVPR 2024]** ***ADI:*** *Learning Disentangled Identifiers for Action-Customized Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.15841.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://adi-t2i.github.io/ADI/)

* **[CVPR 2024]** ***UFOGen:*** *You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.09257.pdf)

* **[CVPR 2024]** ***Interpret Diffusion:*** *Self-Discovering Interpretable Diffusion Latent Directions for Responsible Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.17216.pdf)

* **[CVPR 2024]** ***Tailored Visions:*** *Enhancing Text-to-Image Generation with Personalized Prompt Rewriting*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.08129.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zzjchen/Tailored-Visions)

* **[CVPR 2024]** ***CoDi:*** *Conditional Diffusion Distillation for Higher-Fidelity and Faster Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.01407.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://fast-codi.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/fast-codi/CoDi) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/MKFMIKU/CoDi)

* **[CVPR 2024]** ***Arbitrary‑Scale Diffusion:*** *Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion Model and Implicit Neural Decoder*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.10255.pdf)

* **[CVPR 2024]** ***Human-Centric Priors:*** *Towards Effective Usage of Human-Centric Priors in Diffusion Models for Text-based Human Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.05239)

* **[CVPR 2024]** ***ElasticDiffusion:*** *Training-free Arbitrary Size Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.18822) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://elasticdiffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/MoayedHajiAli/ElasticDiffusion-official)

* **[CVPR 2024]** ***CosmicMan:*** *A Text-to-Image Foundation Model for Humans*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.01294) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://cosmicman-cvpr2024.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/cosmicman-cvpr2024/CosmicMan)

* **[CVPR 2024]** ***PanFusion:*** *Taming Stable Diffusion for Text to 360° Panorama Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.07949) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://chengzhag.github.io/publication/panfusion) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/chengzhag/PanFusion)

* **[CVPR 2024]** ***Intelligent Grimm:*** *Open-ended Visual Storytelling via Latent Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2306.00973) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://haoningwu3639.github.io/StoryGen_Webpage/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/haoningwu3639/StoryGen)

* **[CVPR 2024]** ***Scalability:*** *On the Scalability of Diffusion-based Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.02883)

* **[CVPR 2024]** ***MuLAn:*** *A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.02790) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://mulan-dataset.github.io/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/mulan-dataset/v1.0)

* **[CVPR 2024]** ***Multi-dimensional Preferences:*** *Learning Multi-dimensional Human Preference for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2405.14705)

* **[CVPR 2024]** ***Dynamic Prompts:*** *Dynamic Prompt Optimizing for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.04095)

* **[CVPR 2024]** ***Reinforcement Diversification:*** *Training Diffusion Models Towards Diverse Image Generation with Reinforcement Learning*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Miao_Training_Diffusion_Models_Towards_Diverse_Image_Generation_with_Reinforcement_Learning_CVPR_2024_paper.pdf)

* **[CVPR 2024]** ***HypercGAN:*** *Adversarial Text to Continuous Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Haydarov_Adversarial_Text_to_Continuous_Image_Generation_CVPR_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://kilichbek.github.io/webpage/hypercgan/)

* **[CVPR 2024]** ***EmoGen:*** *Emotional Image Content Generation with Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_EmoGen_Emotional_Image_Content_Generation_with_Text-to-Image_Diffusion_Models_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/JingyuanYY/EmoGen)

* **[ECCV 2024]** ***LaVi‑Bridge:*** *Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.07860) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://shihaozhaozsh.github.io/LaVi-Bridge/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ShihaoZhaoZSH/LaVi-Bridge)

* **[ECCV 2024]** ***DiffPNG:*** *Exploring Phrase-Level Grounding with Text-to-Image Diffusion Model*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2407.05352v1) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/nini0919/DiffPNG)

* **[ECCV 2024]** ***SPRIGHT:*** *Getting it Right: Improving Spatial Consistency in Text-to-Image Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.01197) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://spright-t2i.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/SPRIGHT-T2I/SPRIGHT)

* **[ECCV 2024]** ***IndicTTI:*** *Navigating Text-to-Image Generative Bias across Indic Languages*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.00283v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://iab-rubric.org/resources/other-databases/indictti)

* **[ECCV 2024]** ***Safeguard T2I:*** *Safeguard Text-to-Image Diffusion Models with Human Feedback Inversion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2407.21032)

* **[ECCV 2024]** ***Reality-and-Fantasy:*** *The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2407.12579) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://leo81005.github.io/Reality-and-Fantasy/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://leo81005.github.io/Reality-and-Fantasy/)

* **[ECCV 2024]** ***RECE:*** *Reliable and Efficient Concept Erasure of Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2407.12383v1) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/CharlesGong12/RECE)

* **[ECCV 2024]** ***StyleTokenizer:*** *Defining Image Style by a Single Instance for Controlling Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2409.02543) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/alipay/style-tokenizer)

* **[ECCV 2024]** ***PEA-Diffusion:*** *Parameter-Efficient Adapter with Knowledge Distillation in non-English Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08492.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/OPPO-Mente-Lab/PEA-Diffusion)

* **[ECCV 2024]** ***Skewed Relations T2I:*** *Skews in the Phenomenon Space Hinder Generalization in Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11936.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zdxdsw/skewed_relations_T2I)

* **[ECCV 2024]** ***Parrot:*** *Pareto-optimal Multi-Reward Reinforcement Learning Framework for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05562.pdf)

* **[ECCV 2024]** ***MobileDiffusion:*** *Instant Text-to-Image Generation on Mobile Devices*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07923.pdf)

* **[ECCV 2024]** ***PixArt-Σ:*** *Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.04692) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pixart-alpha.github.io/PixArt-sigma-project/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/PixArt-alpha/PixArt-sigma)

* **[ECCV 2024]** ***CogView3:*** *Finer and Faster Text-to-Image Generation via Relay Diffusion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.05121) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/THUDM/CogView)

* **[ICLR 2024]** ***Patched Diffusion Models:*** *Patched Denoising Diffusion Models For High-Resolution Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2308.01316.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mlpc-ucsd/patch-dm)

* **[ICLR 2024]** ***Relay Diffusion:*** *Unifying diffusion process across resolutions for image synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2309.03350.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/THUDM/RelayDiffusion)

* **[ICLR 2024]** ***SDXL:*** *Improving Latent Diffusion Models for High-Resolution Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2307.01952.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Stability-AI/generative-models)

* **[ICLR 2024]** ***Compose and Conquer:*** *Diffusion-Based 3D Depth Aware Composable Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2401.09048.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/tomtom1103/compose-and-conquer)

* **[ICLR 2024]** ***PixArt-α:*** *Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.00426.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pixart-alpha.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/PixArt-alpha/PixArt-alpha) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/PixArt-alpha/PixArt-alpha)

* **[SIGGRAPH 2024]** ***RGB↔X:*** *Image Decomposition and Synthesis Using Material- and Lighting-aware Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://zheng95z.github.io/assets/files/sig24-rgbx.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zheng95z.github.io/publications/rgbx24)

* **[AAAI 2024]** ***Semantic-aware Augmentation:*** *Semantic-aware Data Augmentation for Text-to-image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.07951.pdf)

* **[AAAI 2024]** ***Abstract Concepts:*** *Text-to-Image Generation for Abstract Concepts*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://ojs.aaai.org/index.php/AAAI/article/view/28122)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

- [Text-to-Image GAN with Pretrained Representations](http://arxiv.org/abs/2501.00116v1)
- [VMix: Improving Text-to-Image Diffusion Model with Cross-Attention Mixing Control](http://arxiv.org/abs/2412.20800v1) [![GitHub Stars](https://img.shields.io/github/stars/fenfenfenfan/VMix?style=social)](https://github.com/fenfenfenfan/VMix)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://vmix-diffusion.github.io/VMix/)
- [INFELM: In-depth Fairness Evaluation of Large Text-To-Image Models](http://arxiv.org/abs/2501.01973v3)
- [Is Your Text-to-Image Model Robust to Caption Noise?](http://arxiv.org/abs/2412.19531v1)
- [DebiasDiff: Debiasing Text-to-image Diffusion Models with Self-discovering Latent Attribute Directions](http://arxiv.org/abs/2412.18810v1) [![GitHub Stars](https://img.shields.io/github/stars/leigest519/DebiasDiff?style=social)](https://github.com/leigest519/DebiasDiff)
- [Explaining in Diffusion: Explaining a Classifier Through Hierarchical Semantics with Text-to-Image Diffusion Models](http://arxiv.org/abs/2412.18604v1)  [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://explain-in-diffusion.github.io/)
- [FameBias: Embedding Manipulation Bias Attack in Text-to-Image Models](http://arxiv.org/abs/2412.18302v1)
- [EvalMuse-40K: A Reliable and Fine-Grained Benchmark with Comprehensive Human Annotations for Text-to-Image Generation Model Evaluation](http://arxiv.org/abs/2412.18150v2)  [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://shh-han.github.io/EvalMuse-project/)  [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/datasets/DY-Evalab/EvalMuse)
- [AEIOU: A Unified Defense Framework against NSFW Prompts in Text-to-Image Models](http://arxiv.org/abs/2412.18123v1)
- [Self-Corrected Flow Distillation for Consistent One-Step and Few-Step Text-to-Image Generation](http://arxiv.org/abs/2412.16906v2)
- [PromptLA: Towards Integrity Verification of Black-box Text-to-Image Diffusion Models](http://arxiv.org/abs/2412.16257v2)
- [GALOT: Generative Active Learning via Optimizable Zero-shot Text-to-image Generation](http://arxiv.org/abs/2412.16227v1)
- [What makes a good metric? Evaluating automatic metrics for text-to-image consistency](http://arxiv.org/abs/2412.13989v1)
- [Maybe you are looking for CroQS: Cross-modal Query Suggestion for Text-to-Image Retrieval](http://arxiv.org/abs/2412.13834v1)
- [CoMPaSS: Enhancing Spatial Understanding in Text-to-Image Diffusion Models](http://arxiv.org/abs/2412.13195v2) [![GitHub Stars](https://img.shields.io/github/stars/blurgyy/CoMPaSS?style=social)](https://github.com/blurgyy/CoMPaSS)  [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/blurgy/CoMPaSS-FLUX.1)
- [ArtAug: Enhancing Text-to-Image Generation through Synthesis-Understanding Interaction](http://arxiv.org/abs/2412.12888v2)  [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/ECNU-CILab/ArtAug-lora-FLUX.1dev-v1)
- [A Framework for Critical Evaluation of Text-to-Image Models: Integrating Art Historical Analysis, Artistic Exploration, and Critical Prompt Engineering](http://arxiv.org/abs/2412.12774v1)
- [Efficient Scaling of Diffusion Transformers for Text-to-Image Generation](http://arxiv.org/abs/2412.12391v1)
- [VersaGen: Unleashing Versatile Visual Control for Text-to-Image Synthesis](http://arxiv.org/abs/2412.11594v3) [![GitHub Stars](https://img.shields.io/github/stars/FelixChan9527/VersaGen_official?style=social)](https://github.com/FelixChan9527/VersaGen_official)
- [Finding a Wolf in Sheep's Clothing: Combating Adversarial Text-To-Image Prompts with Text Summarization](http://arxiv.org/abs/2412.12212v1)
- [AlignGuard: Scalable Safety Alignment for Text-to-Image Generation](http://arxiv.org/abs/2412.10493v2) [![GitHub Stars](https://img.shields.io/github/stars/Visualignment/SafetyDPO?style=social)](https://github.com/Visualignment/SafetyDPO)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://alignguard.github.io/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/Visualignment/safe-stable-diffusion-v1-5)
- [SnapGen: Taming High-Resolution Text-to-Image Models for Mobile Devices with Efficient Architectures and Training](http://arxiv.org/abs/2412.09619v1)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://snap-research.github.io/snapgen/)
- [Context Canvas: Enhancing Text-to-Image Diffusion Models with Knowledge Graph-Based RAG](http://arxiv.org/abs/2412.09614v1)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://context-canvas.github.io/)
- [DECOR:Decomposition and Projection of Text Embeddings for Text-to-Image Customization](http://arxiv.org/abs/2412.09169v1)
- [Fast Prompt Alignment for Text-to-Image Generation](http://arxiv.org/abs/2412.08639v1) [![GitHub Stars](https://img.shields.io/github/stars/tiktok/fast_prompt_alignment?style=social)](https://github.com/tiktok/fast_prompt_alignment)
- [FiVA: Fine-grained Visual Attribute Dataset for Text-to-Image Diffusion Models](http://arxiv.org/abs/2412.07674v1)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://fiva-dataset.github.io/)
- [Preference Adaptive and Sequential Text-to-Image Generation](http://arxiv.org/abs/2412.10419v2)
- [Boosting Alignment for Post-Unlearning Text-to-Image Generative Models](http://arxiv.org/abs/2412.07808v2) [![GitHub Stars](https://img.shields.io/github/stars/reds-lab/Restricted_gradient_diversity_unlearning?style=social)](https://github.com/reds-lab/Restricted_gradient_diversity_unlearning)
- [Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty](http://arxiv.org/abs/2412.06771v2) [![GitHub Stars](https://img.shields.io/github/stars/google-deepmind/proactive_t2i_agents?style=social)](https://github.com/google-deepmind/proactive_t2i_agents)
- [SILMM: Self-Improving Large Multimodal Models for Compositional Text-to-Image Generation](http://arxiv.org/abs/2412.05818v2) [![GitHub Stars](https://img.shields.io/github/stars/LgQu/SILMM?style=social)](https://github.com/LgQu/SILMM)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://silmm.github.io/)
- [Evaluating Hallucination in Text-to-Image Diffusion Models with Scene-Graph based Question-Answering Agent](http://arxiv.org/abs/2412.05722v1)
- [SleeperMark: Towards Robust Watermark against Fine-Tuning Text-to-image Diffusion Models](http://arxiv.org/abs/2412.04852v2) [![GitHub Stars](https://img.shields.io/github/stars/taco-group/SleeperMark?style=social)](https://github.com/taco-group/SleeperMark)
- [LayerFusion: Harmonized Multi-Layer Text-to-Image Generation with Generative Priors](http://arxiv.org/abs/2412.04460v1)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://layerfusion.github.io/)
- [T2I-FactualBench: Benchmarking the Factuality of Text-to-Image Models with Knowledge-Intensive Concepts](http://arxiv.org/abs/2412.04300v3)
- [BodyMetric: Evaluating the Realism of Human Bodies in Text-to-Image Generation](http://arxiv.org/abs/2412.04086v2)
- [Safeguarding Text-to-Image Generation via Inference-Time Prompt-Noise Optimization](http://arxiv.org/abs/2412.03876v1)
- [DynamicControl: Adaptive Condition Selection for Improved Text-to-Image Generation](http://arxiv.org/abs/2412.03255v2) [![GitHub Stars](https://img.shields.io/github/stars/hithqd/DynamicControl?style=social)](https://github.com/hithqd/DynamicControl)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://hithqd.github.io/projects/Dynamiccontrol/)
- [The Role of Text-to-Image Models in Advanced Style Transfer Applications: A Case Study with DALL-E 3](http://arxiv.org/abs/2412.05325v1)
- [Towards Understanding and Quantifying Uncertainty for Text-to-Image Generation](http://arxiv.org/abs/2412.03178v1)
- [ShapeWords: Guiding Text-to-Image Synthesis with 3D Shape-Aware Prompts](http://arxiv.org/abs/2412.02912v1)
- [ScImage: How Good Are Multimodal Large Language Models at Scientific Text-to-Image Generation?](http://arxiv.org/abs/2412.02368v1)
- [Cross-Attention Head Position Patterns Can Align with Human Visual Concepts in Text-to-Image Generative Models](http://arxiv.org/abs/2412.02237v3)
- [Generative Photography: Scene-Consistent Camera Control for Realistic Text-to-Image Synthesis](http://arxiv.org/abs/2412.02168v3)
- [Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis](http://arxiv.org/abs/2412.01819v4)
- [Continuous Concepts Removal in Text-to-image Diffusion Models](http://arxiv.org/abs/2412.00580v2)
- [Blind Inverse Problem Solving Made Easy by Text-to-Image Latent Diffusion](http://arxiv.org/abs/2412.00557v1)
- [Safety Alignment Backfires: Preventing the Re-emergence of Suppressed Concepts in Fine-tuned Text-to-Image Diffusion Models](http://arxiv.org/abs/2412.00357v1)
- [Sparrow: Data-Efficient Video-LLM with Text-to-Image Augmentation](http://arxiv.org/abs/2411.19951v5)
- [QUOTA: Quantifying Objects with Text-to-Image Models for Any Domain](http://arxiv.org/abs/2411.19534v1)
- [DreamBlend: Advancing Personalized Fine-tuning of Text-to-Image Diffusion Models](http://arxiv.org/abs/2411.19390v1)
- [EFSA: Episodic Few-Shot Adaptation for Text-to-Image Retrieval](http://arxiv.org/abs/2412.00139v2)
- [Bridging the Gap: Aligning Text-to-Image Diffusion Models with Specific Feedback](http://arxiv.org/abs/2412.00122v1)
- [Self-Cross Diffusion Guidance for Text-to-Image Synthesis of Similar Subjects](http://arxiv.org/abs/2411.18936v2)
- [All Seeds Are Not Equal: Enhancing Compositional Text-to-Image Generation with Reliable Random Seeds](http://arxiv.org/abs/2411.18810v5)
- [An indicator for effectiveness of text-to-image guardrails utilizing the Single-Turn Crescendo Attack (STCA)](http://arxiv.org/abs/2411.18699v1)
- [Enhancing MMDiT-Based Text-to-Image Models for Similar Subject Generation](http://arxiv.org/abs/2411.18301v1)
- [Type-R: Automatically Retouching Typos for Text-to-Image Generation](http://arxiv.org/abs/2411.18159v2)
- [Reward Incremental Learning in Text-to-Image Generation](http://arxiv.org/abs/2411.17310v1)
- [ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting](http://arxiv.org/abs/2411.17176v1)
- [Relations, Negations, and Numbers: Looking for Logic in Generative Text-to-Image Models](http://arxiv.org/abs/2411.17066v1)
- [Noise Diffusion for Enhancing Semantic Faithfulness in Text-to-Image Synthesis](http://arxiv.org/abs/2411.16503v1)
- [Unlocking the Potential of Text-to-Image Diffusion with PAC-Bayesian Theory](http://arxiv.org/abs/2411.17472v1)
- [CoCoNO: Attention Contrast-and-Complete for Initial Noise Optimization in Text-to-Image Synthesis](http://arxiv.org/abs/2411.16783v1)
- [Text-to-Image Synthesis: A Decade Survey](http://arxiv.org/abs/2411.16164v1)
- [In-Context Experience Replay Facilitates Safety Red-Teaming of Text-to-Image Diffusion Models](http://arxiv.org/abs/2411.16769v2)


</details>

</details>

<details>
<summary><h4>✨ 2023</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2023]** ***GigaGAN:*** *Scaling Up GANs for Text-to-Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Scaling_Up_GANs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://mingukkang.github.io/GigaGAN/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lucidrains/gigagan-pytorch)

* **[CVPR 2023]** ***ERNIE-ViLG 2.0:*** *Improving Text-to-Image Diffusion Model With Knowledge-Enhanced Mixture-of-Denoising-Experts*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_ERNIE-ViLG_2.0_Improving_Text-to-Image_Diffusion_Model_With_Knowledge-Enhanced_Mixture-of-Denoising-Experts_CVPR_2023_paper.pdf)

* **[CVPR 2023]** ***Shifted Diffusion:*** *Shifted Diffusion for Text-to-image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Shifted_Diffusion_for_Text-to-Image_Generation_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/drboog/Shifted_Diffusion)

* **[CVPR 2023]** ***GALIP:*** *Generative Adversarial CLIPs for Text-to-Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tao_GALIP_Generative_Adversarial_CLIPs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/tobran/GALIP)

* **[CVPR 2023]** ***Specialist Diffusion:*** *Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image Diffusion Models to Learn Any Unseen Style*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Specialist_Diffusion_Plug-and-Play_Sample-Efficient_Fine-Tuning_of_Text-to-Image_Diffusion_Models_To_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Picsart-AI-Research/Specialist-Diffusion)

* **[CVPR 2023]** ***Verifiable Evaluation:*** *Toward Verifiable and Reproducible Human Evaluation for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Otani_Toward_Verifiable_and_Reproducible_Human_Evaluation_for_Text-to-Image_Generation_CVPR_2023_paper.pdf)

* **[CVPR 2023]** ***RIATIG:*** *Reliable and Imperceptible Adversarial Text-to-Image Generation with Natural Prompts*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_RIATIG_Reliable_and_Imperceptible_Adversarial_Text-to-Image_Generation_With_Natural_Prompts_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/WUSTL-CSPL/RIATIG)

* **[CVPR 2023]** ***Custom Diffusion:*** *Multi-Concept Customization of Text-to-Image Diffusion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kumari_Multi-Concept_Customization_of_Text-to-Image_Diffusion_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://www.cs.cmu.edu/~custom-diffusion/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/adobe-research/custom-diffusion)

* **[ICCV 2023]** ***DiffFit:*** *Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_DiffFit_Unlocking_Transferability_of_Large_Diffusion_Models_via_Simple_Parameter-efficient_ICCV_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mkshing/DiffFit-pytorch)

* **[NeurIPS 2023]** ***ImageReward:*** *Learning and Evaluating Human Preferences for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=JVzeOYEx6d) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/THUDM/ImageReward)

* **[NeurIPS 2023]** ***RAPHAEL:*** *Text-to-Image Generation via Large Mixture of Diffusion Paths*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.18295) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://raphael-painter.github.io/)

* **[NeurIPS 2023]** ***Linguistic Binding:*** *Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=AOKU4nRw1W) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/RoyiRa/Linguistic-Binding-in-Diffusion-Models)

* **[NeurIPS 2023]** ***DenseDiffusion:*** *Dense Text-to-Image Generation with Attention Modulation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Dense_Text-to-Image_Generation_with_Attention_Modulation_ICCV_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/naver-ai/densediffusion)

* **[ICLR 2023]** ***Structured Diffusion Guidance:*** *Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=PUIqjT4rzq7) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/weixi-feng/Structured-Diffusion-Guidance)

* **[ICML 2023]** ***StyleGAN-T:*** *Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.mlr.press/v202/sauer23a/sauer23a.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sites.google.com/view/stylegan-t/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/autonomousvision/stylegan-t)

* **[ICML 2023]** ***Muse:*** *Text-To-Image Generation via Masked Generative Transformers*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.mlr.press/v202/chang23b/chang23b.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://muse-icml.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lucidrains/muse-maskgit-pytorch)

* **[ICML 2023]** ***UniDiffusers:*** *One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2303.06555) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/thu-ml/unidiffuser)

* **[ACM MM 2023]** ***SUR-adapter:*** *Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.05189.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Qrange-group/SUR-adapter)

* **[ACM MM 2023]** ***ControlStyle:*** *Text-Driven Stylized Image Generation Using Diffusion Priors*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.05463.pdf)

* **[SIGGRAPH 2023]** ***Attend-and-Excite:*** *Attention-Based Semantic Guidance for Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2301.13826.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yuval-alaluf.github.io/Attend-and-Excite/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/yuval-alaluf/Attend-and-Excite) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/AttendAndExcite/Attend-and-Excite)



</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

[<small>⇧ Back to ToC</small>](#contents)

### <span id="conditional">🕹️ Conditional Image Generation</span>

<details>
<summary><h4>✨ 2025</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2024]** ***PLACE:*** *Adaptive Layout‑Semantic Fusion for Semantic Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.01852.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/cszy98/PLACE)

* **[CVPR 2024]** ***One‑Shot Structure‑Aware Stylized Image Synthesis:*** *One‑Shot Structure‑Aware Stylized Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.17275.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/hansam95/OSASIS)

* **[CVPR 2024]** ***Attention Refocusing:*** *Grounded Text‑to‑Image Synthesis with Attention Refocusing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2306.05427.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://attention-refocusing.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Attention-Refocusing/attention-refocusing) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/attention-refocusing/Attention-refocusing)

* **[CVPR 2024]** ***CFLD:*** *Coarse‑to‑Fine Latent Diffusion for Pose‑Guided Person Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.18078.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/YanzuoLu/CFLD)

* **[CVPR 2024]** ***DetDiffusion:*** *Synergizing Generative and Perceptive Models for Enhanced Data Generation and Perception*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.13304)

* **[CVPR 2024]** ***CAN:*** *Condition‑Aware Neural Network for Controlled Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.01143.pdf)

* **[CVPR 2024]** ***SceneDiffusion:*** *Move Anything with Layered Scene Diffusion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.07178)

* **[CVPR 2024]** ***Zero‑Painter:*** *Training‑Free Layout Control for Text‑to‑Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Ohanyan_Zero-Painter_Training-Free_Layout_Control_for_Text-to-Image_Synthesis_CVPR_2024_paper.pdf) 

* **[CVPR 2024]** ***MIGC:*** *Multi‑Instance Generation Controller for Text‑to‑Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_MIGC_Multi-Instance_Generation_Controller_for_Text-to-Image_Synthesis_CVPR_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://migcproject.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/limuloo/MIGC)

* **[CVPR 2024]** ***FreeControl:*** *Training‑Free Spatial Control of Any Text‑to‑Image Diffusion Model with Any Condition*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Mo_FreeControl_Training-Free_Spatial_Control_of_Any_Text-to-Image_Diffusion_Model_with_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/genforce/freecontrol)

* **[ECCV 2024]** ***PreciseControl:*** *Enhancing Text‑To‑Image Diffusion Models with Fine‑Grained Attribute Control*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.05083) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://rishubhpar.github.io/PreciseControl.home/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/rishubhpar/PreciseControl)

* **[ECCV 2024]** ***AnyControl:*** *Create Your Artwork with Versatile Control on Text‑to‑Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01706.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/open-mmlab/AnyControl)

* **[NeurIPS 2024]** ***Ctrl‑X:*** *Controlling Structure and Appearance for Text‑To‑Image Generation Without Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2406.07540) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://genforce.github.io/ctrl-x/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/genforce/ctrl-x)

* **[ICLR 2024]** ***PCDMs:*** *Advancing Pose‑Guided Image Synthesis with Progressive Conditional Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.06313.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/muzishen/PCDMs)

* **[WACV 2024]** ***Layout Control with Cross‑Attention Guidance:*** *Training‑Free Layout Control with Cross‑Attention Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Training-Free_Layout_Control_With_Cross-Attention_Guidance_WACV_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://silent-chen.github.io/layout-guidance/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/silent-chen/layout-guidance) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/silentchen/layout-guidance)

* **[AAAI 2024]** ***SSMG:*** *Spatial‑Semantic Map Guided Diffusion Model for Free‑form Layout‑to‑image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2308.10156.pdf)

* **[AAAI 2024]** ***Attention Map Control:*** *Compositional Text‑to‑Image Synthesis with Attention Map Control of Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.13921.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/OPPO-Mente-Lab/attention-mask-control)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>✨ 2024</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2024]** ***PLACE:*** *Adaptive Layout‑Semantic Fusion for Semantic Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.01852.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/cszy98/PLACE)

* **[CVPR 2024]** ***One‑Shot Structure‑Aware Stylized Image Synthesis:*** *One‑Shot Structure‑Aware Stylized Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.17275.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/hansam95/OSASIS)

* **[CVPR 2024]** ***Attention Refocusing:*** *Grounded Text‑to‑Image Synthesis with Attention Refocusing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2306.05427.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://attention-refocusing.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Attention-Refocusing/attention-refocusing) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/attention-refocusing/Attention-refocusing)

* **[CVPR 2024]** ***CFLD:*** *Coarse‑to‑Fine Latent Diffusion for Pose‑Guided Person Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.18078.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/YanzuoLu/CFLD)

* **[CVPR 2024]** ***DetDiffusion:*** *Synergizing Generative and Perceptive Models for Enhanced Data Generation and Perception*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.13304) 

* **[CVPR 2024]** ***CAN:*** *Condition‑Aware Neural Network for Controlled Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.01143.pdf)

* **[CVPR 2024]** ***SceneDiffusion:*** *Move Anything with Layered Scene Diffusion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.07178)

* **[CVPR 2024]** ***Zero‑Painter:*** *Training‑Free Layout Control for Text‑to‑Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Ohanyan_Zero-Painter_Training-Free_Layout_Control_for_Text-to-Image_Synthesis_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Picsart-AI-Research/Zero-Painter)

* **[CVPR 2024]** ***MIGC:*** *Multi‑Instance Generation Controller for Text‑to‑Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_MIGC_Multi-Instance_Generation_Controller_for_Text-to-Image_Synthesis_CVPR_2024_paper.pdf) [![Project Page](https://img.shields.io-badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://migcproject.github.io/) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/limuloo/MIGC)

* **[CVPR 2024]** ***FreeControl:*** *Training‑Free Spatial Control of Any Text‑to‑Image Diffusion Model with Any Condition*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Mo_FreeControl_Training-Free_Spatial_Control_of_Any_Text-to-Image_Diffusion_Model_with_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/genforce/freecontrol)

* **[ECCV 2024]** ***PreciseControl:*** *Enhancing Text‑To‑Image Diffusion Models with Fine‑Grained Attribute Control*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.05083) [![Project Page](https://img.shields.io-badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://rishubhpar.github.io/PreciseControl.home/) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/rishubhpar/PreciseControl) 

* **[ECCV 2024]** ***AnyControl:*** *Create Your Artwork with Versatile Control on Text‑to‑Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01706.pdf) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/open-mmlab/AnyControl) 

* **[NeurIPS 2024]** ***Ctrl‑X:*** *Controlling Structure and Appearance for Text‑To‑Image Generation Without Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2406.07540) [![Project Page](https://img.shields.io-badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://genforce.github.io/ctrl-x/) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/genforce/ctrl-x) 

* **[ICLR 2024]** ***PCDMs:*** *Advancing Pose‑Guided Image Synthesis with Progressive Conditional Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.06313.pdf) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/muzishen/PCDMs) 

* **[WACV 2024]** ***Layout Control with Cross‑Attention Guidance:*** *Training‑Free Layout Control with Cross‑Attention Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Training-Free_Layout_Control_With_Cross-Attention_Guidance_WACV_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://silent-chen.github.io/layout-guidance/) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/silent-chen/layout-guidance) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/silentchen/layout-guidance)

* **[AAAI 2024]** ***SSMG:*** *Spatial‑Semantic Map Guided Diffusion Model for Free‑form Layout‑to‑image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2308.10156.pdf)

* **[AAAI 2024]** ***Attention Map Control:*** *Compositional Text‑to‑Image Synthesis with Attention Map Control of Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.13921.pdf) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/OPPO-Mente-Lab/attention-mask-control)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>✨ 2023</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2023]** ***GLIGEN:*** *Open-Set Grounded Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_GLIGEN_Open-Set_Grounded_Text-to-Image_Generation_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://gligen.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/gligen/GLIGEN) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/gligen/demo)

* **[CVPR 2022]** ***Autoregressive Image Generation:*** *Using Residual Quantization*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2022/papers/Lee_Autoregressive_Image_Generation_Using_Residual_Quantization_CVPR_2022_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/kakaobrain/rq-vae-transformer)

* **[CVPR 2023]** ***SpaText:*** *Spatio-Textual Representation for Controllable Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Avrahami_SpaText_Spatio-Textual_Representation_for_Controllable_Image_Generation_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://omriavrahami.com/spatext/)

* **[CVPR 2022]** ***Text to Image Generation with Semantic-Spatial Aware GAN:*** *Text to Image Generation with Semantic-Spatial Aware GAN*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2022/papers/Liao_Text_to_Image_Generation_With_Semantic-Spatial_Aware_GAN_CVPR_2022_paper.pdf)

* **[CVPR 2023]** ***ReCo:*** *Region-Controlled Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_ReCo_Region-Controlled_Text-to-Image_Generation_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/microsoft/ReCo)

* **[CVPR 2023]** ***LayoutDiffusion:*** *Controllable Diffusion Model for Layout-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_LayoutDiffusion_Controllable_Diffusion_Model_for_Layout-to-Image_Generation_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ZGCTroy/LayoutDiffusion)

* **[ICLR 2023]** ***Ctrl-U:*** *Robust Conditional Image Generation via Uncertainty-aware Reward Modeling*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=eC2ICbECNM) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://grenoble-zhang.github.io/Ctrl-U-Page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/grenoble-zhang/Ctrl-U)

* **[ICCV 2023]** ***ControlNet:*** *Adding Conditional Control to Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Adding_Conditional_Control_to_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lllyasviel/ControlNet) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/lllyasviel/ControlNet)

* **[ICCV 2023]** ***SceneGenie:*** *Scene Graph Guided Diffusion Models for Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023W/SG2RL/papers/Farshad_SceneGenie_Scene_Graph_Guided_Diffusion_Models_for_Image_Synthesis_ICCVW_2023_paper.pdf)

* **[ICCV 2023]** ***ZestGuide:*** *Zero-Shot Spatial Layout Conditioning for Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Couairon_Zero-Shot_Spatial_Layout_Conditioning_for_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf)

* **[ICML 2023]** ***Composer:*** *Creative and Controllable Image Synthesis with Composable Conditions*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.mlr.press/v202/huang23b/huang23b.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ali-vilab.github.io/composer-page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ali-vilab/composer)

* **[ICML 2023]** ***MultiDiffusion:*** *Fusing Diffusion Paths for Controlled Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.mlr.press/v202/bar-tal23a/bar-tal23a.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://multidiffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/omerbt/MultiDiffusion) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/weizmannscience/MultiDiffusion)

* **[SIGGRAPH 2023]** ***Sketch-Guided Text-to-Image Diffusion Models:*** *Sketch-Guided Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://dl.acm.org/doi/pdf/10.1145/3588432.3591560) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sketch-guided-diffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ogkalu2/Sketch-Guided-Stable-Diffusion)

* **[NeurIPS 2023]** ***Uni-ControlNet:*** *All-in-One Control to Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.16322.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://shihaozhaozsh.github.io/unicontrolnet/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ShihaoZhaoZSH/Uni-ControlNet)

* **[NeurIPS 2023]** ***Prompt Diffusion:*** *In-Context Learning Unlocked for Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=6BZS2EAkns) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zhendong-wang.github.io/prompt-diffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Zhendong-Wang/Prompt-Diffusion)

* **[WACV 2023]** ***More Control for Free!:*** *Image Synthesis with Semantic Diffusion Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/WACV2023/papers/Liu_More_Control_for_Free_Image_Synthesis_With_Semantic_Diffusion_Guidance_WACV_2023_paper.pdf)

* **[ACM MM 2023]** ***LayoutLLM-T2I:*** *Eliciting Layout Guidance from LLM for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2308.05095.pdf)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

[<small>⇧ Back to ToC</small>](#contents)

### <span id="personalized">🎨 Personalized Image Generation</span>

<details>
<summary><h4>✨ 2025</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2025]** ***SerialGen:*** *Personalized Image Generation by First Standardization Then Personalization*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.01485) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://serialgen.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/)

* **[CVPR 2025]** ***PatchDPO:*** *Patch-level DPO for Finetuning-free Personalized Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.03177) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://github.com/hqhQAQ/PatchDPO) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/hqhQAQ/PatchDPO) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/hqhQAQ/PatchDPO)

* **[CVPR 2025]** ***DreamCache:*** *Finetuning-Free Lightweight Personalized Image Generation via Feature Caching*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.17786) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://emanuele97x.github.io/DreamCache) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Emanuele97x/DreamCache)

* **[NeurIPS 2025]** ***MS-Diffusion:*** *Multi-Subject Zero-shot Image Personalization with Layout Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2406.07209) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ms-diffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/MS-Diffusion/MS-Diffusion) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/doge1516/MS-Diffusion)

* **[NeurIPS 2025]** ***ClassDiffusion:*** *More Aligned Personalization Tuning with Explicit Class Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=iTm4H6N4aG) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://classdiffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Rbrq03/ClassDiffusion)

* **[NeurIPS 2025]** ***DreamBench++:*** *A Human-Aligned Benchmark for Personalized Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2406.16855) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://dreambenchplus.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/yuangpeng/dreambench_plus) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/yuangpeng/dreambench_plus)

* **[NeurIPS 2025]** ***TweedieMix:*** *Improving Multi-Concept Fusion for Diffusion-based Image/Video Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2410.05591) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://github.com/KwonGihyun/TweedieMix) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/KwonGihyun/TweedieMix)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>✨ 2024</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2024]** ***Cross Initialization:*** *Personalized Text‑to‑Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.15905.pdf)

* **[CVPR 2024]** ***When StyleGAN Meets Stable Diffusion:*** *a W+ Adapter for Personalized Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.17461.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://csxmli2016.github.io/projects/w-plus-adapter/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/csxmli2016/w-plus-adapter)

* **[CVPR 2024]** ***Style Aligned:*** *Image Generation via Shared Attention*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.02133.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://style-aligned-gen.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/google/style-aligned)

* **[CVPR 2024]** ***InstantBooth:*** *Personalized Text‑to‑Image Generation without Test‑Time Finetuning*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2304.03411.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://jshi31.github.io/InstantBooth/)

* **[CVPR 2024]** ***High Fidelity:*** *Person‑centric Subject‑to‑Image Synthesis*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.10329.pdf)

* **[CVPR 2024]** ***RealCustom:*** *Narrowing Real Text Word for Real‑Time Open‑Domain Text‑to‑Image Customization*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.00483.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://corleone-huang.github.io/realcustom/) [![🤗 Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/bytedance-research/RealCustom)

* **[CVPR 2024]** ***DisenDiff:*** *Attention Calibration for Disentangled Text‑to‑Image Personalization*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.18551) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Monalissaa/DisenDiff)

* **[CVPR 2024]** ***FreeCustom:*** *Tuning‑Free Customized Image Generation for Multi‑Concept Composition*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2405.13870v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://aim-uofa.github.io/FreeCustom/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/aim-uofa/FreeCustom)

* **[CVPR 2024]** ***Personalized Residuals:*** *for Concept‑Driven Text‑to‑Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2405.12978)

* **[CVPR 2024]** ***Subject‑Agnostic Guidance:*** *Improving Subject‑Driven Image Synthesis*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2405.01356)

* **[CVPR 2024]** ***JeDi:*** *Joint‑Image Diffusion Models for Finetuning‑Free Personalized Text‑to‑Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zeng_JeDi_Joint-Image_Diffusion_Models_for_Finetuning-Free_Personalized_Text-to-Image_Generation_CVPR_2024_paper.pdf)

* **[CVPR 2024]** ***Influence Watermarks:*** *Countering Personalized Text‑to‑Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Countering_Personalized_Text-to-Image_Generation_with_Influence_Watermarks_CVPR_2024_paper.pdf)

* **[CVPR 2024]** ***PIA:*** *Your Personalized Image Animator via Plug‑and‑Play Modules in Text‑to‑Image Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_PIA_Your_Personalized_Image_Animator_via_Plug-and-Play_Modules_in_Text-to-Image_CVPR_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pi-animator.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/open-mmlab/PIA)

* **[CVPR 2024]** ***SSR‑Encoder:*** *Encoding Selective Subject Representation for Subject‑Driven Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_SSR-Encoder_Encoding_Selective_Subject_Representation_for_Subject-Driven_Generation_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Xiaojiu-z/SSR_Encoder)

* **[ECCV 2024]** ***Be Yourself:*** *Bounded Attention for Multi‑Subject Text‑to‑Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.16990) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://omer11a.github.io/bounded-attention/)

* **[ECCV 2024]** ***Powerful and Flexible:*** *Personalized Text‑to‑Image Generation via Reinforcement Learning*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://arxiv.org/pdf/2407.06642v1) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/wfanyue/DPG-T2I-Personalization)

* **[ECCV 2024]** ***TIGC:*** *Tuning‑Free Image Customization with Image and Text Guidance*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.12658) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zrealli.github.io/TIGIC/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zrealli/TIGIC)

* **[ECCV 2024]** ***MasterWeaver:*** *Taming Editability and Face Identity for Personalized Text‑to‑Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06786.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://masterweaver.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/csyxwei/MasterWeaver)

* **[NeurIPS 2024]** ***RectifID:*** *Personalizing Rectified Flow with Anchored Classifier Guidance*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.neurips.cc/paper_files/paper/2024/file/afa58a5b6adc0845e0fd632132a64c39-Paper-Conference.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/feifeiobama/RectifID)

* **[NeurIPS 2024]** ***AttnDreamBooth:*** *Towards Text‑Aligned Personalized Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.neurips.cc/paper_files/paper/2024/file/465a13a95741fab2e912f98adb07df1d-Paper-Conference.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://attndreambooth.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lyuPang/AttnDreamBooth)

* **[AAAI 2024]** ***Decoupled Textual Embeddings:*** *for Customized Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.11826.pdf)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>✨ 2023</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2023]** ***Custom Diffusion:*** *Multi-Concept Customization of Text-to-Image Diffusion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kumari_Multi-Concept_Customization_of_Text-to-Image_Diffusion_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://www.cs.cmu.edu/~custom-diffusion/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/adobe-research/custom-diffusion) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/custom-diffusion-library/cat)

* **[CVPR 2023]** ***DreamBooth:*** *Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://dreambooth.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/google/dreambooth) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/BAAI/DreamBooth-AltDiffusion)

* **[ICCV 2023]** ***ELITE:*** *Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_ELITE_Encoding_Visual_Concepts_into_Textual_Embeddings_for_Customized_Text-to-Image_ICCV_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://github.com/csyxwei/ELITE) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/csyxwei/ELITE) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/ELITE-library/ELITE)

* **[ICLR 2023]** ***Textual Inversion:*** *An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=NAQvF08TcyG) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://textual-inversion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/rinongal/textual_inversion) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/Clyuue/textual_inversion_cat)

* **[SIGGRAPH Asia 2023]** ***Break-A-Scene:*** *Extracting Multiple Concepts from a Single Image*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.16311.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://omriavrahami.com/break-a-scene) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/google/break-a-scene)

* **[SIGGRAPH 2023]** ***Encoder‑Based Domain Tuning:*** *Encoder‑Based Domain Tuning for Fast Personalization of Text‑to‑Image Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2302.12228.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://tuning-encoder.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mkshing/e4t-diffusion)

* **[SIGGRAPH 2023]** ***LayerDiffusion:*** *Layered Controlled Image Editing with Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://dl.acm.org/doi/pdf/10.1145/3610543.3626172) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zrealli.github.io/layerdiffusion/index.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lllyasviel/LayerDiffuse) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/LayerDiffusion/layerdiffusion-v1)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

[<small>⇧ Back to ToC</small>](#contents)

### <span id="editing">✂️ Image Editing</span>

<details>
<summary><h4>✨ 2025</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2025]** ***FDS:*** *Frequency‑Aware Denoising Score for Text‑Guided Latent Diffusion Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.19191) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ivrl.github.io/fds-webpage/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/IVRL/FDS)


* **[CVPR 2025]** *Reference‑Based 3D‑Aware Image Editing with Triplanes*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.03632) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://three-bee.github.io/triplane_edit/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/three-bee/triplane_edit)


* **[CVPR 2025]** ***MoEdit:*** *On Learning Quantity Perception for Multi‑object Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.10112)


* **[ICLR 2025]** *Lightning‑Fast Image Inversion and Editing for Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=t9l63huPRt) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://barakmam.github.io/rnri.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/dvirsamuel/NewtonRaphsonInversion)


* **[ICLR 2025]** *Multi‑Reward as Condition for Instruction‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=9RFocgIccP) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/bytedance/Multi-Reward-Editing)


* **[ICLR 2025]** ***HQ‑Edit:*** *A High‑Quality Dataset for Instruction‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=mZptYYttFj) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://thefllood.github.io/HQEdit_web/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/UCSC-VLAA/HQ-Edit) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/UCSC-VLAA/HQ-Edit)


* **[ICLR 2025]** ***CLIPDrag:*** *Combining Text‑based and Drag‑based Instructions for Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=2HjRezQ1nj) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/HKUST-LongGroup/CLIPDrag)


* **[ICLR 2025]** *Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=Hu0FSOSEyS) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://rf-inversion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LituRout/RF-Inversion)


* **[ICLR 2025]** ***PostEdit:*** *Posterior Sampling for Efficient Zero‑Shot Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=J8YWCBPgx7) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/TFNTF/PostEdit)


* **[ICLR 2025]** ***OmniEdit:*** *Building Image Editing Generalist Models Through Specialist Supervision*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=Hlm0cga0sv) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://tiger-ai-lab.github.io/OmniEdit/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/TIGER-AI-Lab/OmniEdit) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/TIGER-Lab/OmniEdit-Filtered-1.2M)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>✨ 2024</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2024]** ***InfEdit:*** *Inversion‑Free Image Editing with Natural Language*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.04965.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sled-group.github.io/InfEdit/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/sled-group/InfEdit)


* **[CVPR 2024]** ***CrossSelfAttention:*** *Towards Understanding Cross and Self‑Attention in Stable Diffusion for Text‑Guided Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.03431.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/alibaba/EasyNLP/tree/master/diffusion/FreePromptEditing)


* **[CVPR 2024]** ***DAC:*** *Doubly Abductive Counterfactual Inference for Text‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.02981.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/xuesong39/DAC)


* **[CVPR 2024]** ***FoI:*** *Focus on Your Instruction: Fine‑grained and Multi‑instruction Image Editing by Attention Modulation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.10113.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/guoqincode/Focus-on-Your-Instruction)


* **[CVPR 2024]** ***CDS:*** *Contrastive Denoising Score for Text‑guided Latent Diffusion Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.18608.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://hyelinnam.github.io/CDS/)


* **[CVPR 2024]** ***DragDiffusion:*** *Harnessing Diffusion Models for Interactive Point‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2306.14435.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yujun-shi.github.io/projects/dragdiffusion.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Yujun-Shi/DragDiffusion)


* **[CVPR 2024]** ***DiffEditor:*** *Boosting Accuracy and Flexibility on Diffusion‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.02583.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/MC-E/DragonDiffusion)


* **[CVPR 2024]** ***FreeDrag:*** *Feature Dragging for Reliable Point‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2307.04684.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LPengYang/FreeDrag)


* **[CVPR 2024]** ***Learnable Regions:*** *Text‑Driven Image Editing via Learnable Regions*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.16432.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yuanze-lin.me/LearnableRegions_page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/yuanze-lin/Learnable_Regions)


* **[CVPR 2024]** ***LEDITS++:*** *Limitless Image Editing using Text‑to‑Image Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.16711.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://leditsplusplus-project.static.hf.space/index.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://huggingface.co/spaces/editing-images/leditsplusplus/tree/main) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/editing-images/leditsplusplus)


* **[CVPR 2024]** ***SmartEdit:*** *Exploring Complex Instruction‑based Image Editing with Large Language Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.06739.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yuzhou914.github.io/SmartEdit/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/TencentARC/SmartEdit)


* **[CVPR 2024]** ***Edit One for All:*** *Interactive Batch Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2401.10219.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://thaoshibe.github.io/edit-one-for-all/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/thaoshibe/edit-one-for-all)


* **[CVPR 2024]** ***DiffMorpher:*** *Unleashing the Capability of Diffusion Models for Image Morphing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.07409.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://kevin-thu.github.io/DiffMorpher_page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Kevin-thu/DiffMorpher)


* **[CVPR 2024]** ***TiNO‑Edit:*** *Timestep and Noise Optimization for Robust Diffusion‑Based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.11120.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/SherryXTChen/TiNO-Edit)


* **[CVPR 2024]** ***Person in Place:*** *Generating Associative Skeleton‑Guidance Maps for Human‑Object Interaction Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Person_in_Place_Generating_Associative_Skeleton-Guidance_Maps_for_Human-Object_Interaction_CVPR_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yangchanghee.github.io/Person-in-Place_page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/YangChangHee/CVPR2024_Person-In-Place_RELEASE)


* **[CVPR 2024]** ***Referring Image Editing:*** *Object‑level Image Editing via Referring Expressions*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Referring_Image_Editing_Object-level_Image_Editing_via_Referring_Expressions_CVPR_2024_paper.pdf)


* **[CVPR 2024]** ***Prompt Augmentation:*** *Prompt Augmentation for Self‑supervised Text‑guided Image Manipulation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Bodur_Prompt_Augmentation_for_Self-supervised_Text-guided_Image_Manipulation_CVPR_2024_paper.pdf)


* **[CVPR 2024]** ***StyleFeatureEditor:*** *The Devil is in the Details — StyleFeatureEditor for Detail‑Rich StyleGAN Inversion and High Quality Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Bobkov_The_Devil_is_in_the_Details_StyleFeatureEditor_for_Detail-Rich_StyleGAN_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/AIRI-Institute/StyleFeatureEditor)


* **[ECCV 2024]** ***RegionDrag:*** *Fast Region‑Based Image Editing with Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://arxiv.org/pdf/2407.18247v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://visual-ai.github.io/regiondrag/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Visual-AI/RegionDrag)


* **[ECCV 2024]** ***TurboEdit:*** *Instant Text‑Based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.08332v1.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://betterze.github.io/TurboEdit/)


* **[ECCV 2024]** ***InstructGIE:*** *Towards Generalizable Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.05018.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://cr8br0ze.github.io/InstructGIE)


* **[ECCV 2024]** ***StableDrag:*** *Stable Dragging for Point‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.04437.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://stabledrag.github.io/)


* **[ECCV 2024]** ***Eta Inversion:*** *Designing an Optimal Eta Function for Diffusion‑based Real Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02157.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/furiosa-ai/eta-inversion)


* **[ECCV 2024]** ***SwapAnything:*** *Enabling Arbitrary Object Swapping in Personalized Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04768.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://swap-anything.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/eric-ai-lab/swap-anything)


* **[ECCV 2024]** ***Guide‑and‑Rescale:*** *Self‑Guidance Mechanism for Effective Tuning‑Free Real Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08987.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/AIRI-Institute/Guide-and-Rescale)


* **[ECCV 2024]** ***FreeDiff:*** *Progressive Frequency Truncation for Image Editing with Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00759.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Thermal-Dynamics/FreeDiff)


* **[ECCV 2024]** ***Lazy Diffusion Transformer:*** *Lazy Diffusion Transformer for Interactive Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03436.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://lazydiffusion.github.io/)


* **[ECCV 2024]** ***ByteEdit:*** *Boost, Comply and Accelerate Generative Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00359.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://byte-edit.github.io/)


* **[ICLR 2024]** ***MGIE:*** *Guiding Instruction‑based Image Editing via Multimodal Large Language Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2309.17102.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://mllm-ie.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/apple/ml-mgie)


* **[ICLR 2024]** ***SDE‑Drag:*** *The Blessing of Randomness — SDE Beats ODE in General Diffusion‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.01410.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ml-gsai.github.io/SDE-Drag-demo/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ML-GSAI/SDE-Drag)


* **[ICLR 2024]** ***Motion Guidance:*** *Diffusion‑Based Image Editing with Differentiable Motion Estimators*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2401.18085.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://dangeng.github.io/motion_guidance/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/dangeng/motion_guidance)


* **[ICLR 2024]** ***OIR:*** *Object‑Aware Inversion and Reassembly for Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.12149.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://aim-uofa.github.io/OIR-Diffusion/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/aim-uofa/OIR)


* **[ICLR 2024]** ***Noise Map Guidance:*** *Inversion with Spatial Context for Real Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.04625.pdf)


* **[AAAI 2024]** ***TIC:*** *Tuning‑Free Inversion‑Enhanced Control for Consistent Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.14611.pdf)


* **[AAAI 2024]** ***BARET:*** *Balanced Attention based Real Image Editing driven by Target‑text Inversion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.05482.pdf)


* **[AAAI 2024]** ***CacheEdit:*** *Accelerating Text‑to‑Image Editing via Cache‑Enabled Sparse Diffusion Inference*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.17423.pdf)


* **[AAAI 2024]** ***High‑Fidelity Editing:*** *High‑Fidelity Diffusion‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.15707.pdf)


* **[AAAI 2024]** ***AdapEdit:*** *Spatio‑Temporal Guided Adaptive Editing Algorithm for Text‑Based Continuity‑Sensitive Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.08019.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/AnonymousPony/adap-edit)


* **[AAAI 2024]** ***TexFit:*** *Text‑Driven Fashion Image Editing with Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://ojs.aaai.org/index.php/AAAI/article/view/28885)

</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>✨ 2023</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2023]** ***Diffusion Disentanglement:*** *Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Uncovering_the_Disentanglement_Capability_in_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://wuqiuche.github.io/DiffusionDisentanglement-project-page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement)

* **[CVPR 2023]** ***SINE:*** *SINgle Image Editing with Text-to-Image Diffusion Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_SINE_SINgle_Image_Editing_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zhang-zx.github.io/SINE/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zhang-zx/SINE)

* **[CVPR 2023]** ***Imagic:*** *Text-Based Real Image Editing with Diffusion Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kawar_Imagic_Text-Based_Real_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://imagic-editing.github.io/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/fffiloni/imagic-stable-diffusion)

* **[CVPR 2023]** ***InstructPix2Pix:*** *Learning to Follow Image Editing Instructions*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Brooks_InstructPix2Pix_Learning_To_Follow_Image_Editing_Instructions_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://www.timothybrooks.com/instruct-pix2pix/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/timothybrooks/instruct-pix2pix) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/timbrooks/instruct-pix2pix)

* **[CVPR 2023]** ***Null-text Inversion:*** *Null-text Inversion for Editing Real Images using Guided Diffusion Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Mokady_NULL-Text_Inversion_for_Editing_Real_Images_Using_Guided_Diffusion_Models_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://null-text-inversion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/google/prompt-to-prompt)

* **[ICCV 2023]** ***MasaCtrl:*** *Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_MasaCtrl_Tuning-Free_Mutual_Self-Attention_Control_for_Consistent_Image_Synthesis_and_ICCV_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ljzycmd.github.io/projects/MasaCtrl/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/TencentARC/MasaCtrl)

* **[ICCV 2023]** ***Local Prompt Mixing:*** *Localizing Object-level Shape Variations with Text-to-Image Diffusion Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Patashnik_Localizing_Object-Level_Shape_Variations_with_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://orpatashnik.github.io/local-prompt-mixing/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/orpatashnik/local-prompt-mixing) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/orpatashnik/local-prompt-mixing)

* **[ICLR 2022]** ***SDEdit:*** *Guided Image Synthesis and Editing with Stochastic Differential Equations*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2108.01073.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sde-image-editing.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ermongroup/SDEdit)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

[<small>⇧ Back to ToC</small>](#contents)



---

## <span id="datasets">🗂️ Datasets</span>
| Dataset Name | Year | Modalities | Task | Paper | Link |
| :--- | :--- | :--- | :--- | :---: | :---: |
| **MS COCO** | 2014 | Text, Image | Text-to-Image Generation, Image Captioning | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/1405.0312) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://cocodataset.org/#home) |
| **Oxford-120 Flowers**| 2008 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://www.robots.ox.ac.uk/~vgg/publications/2008/Nilsback08/nilsback08.pdf) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/) |
| **CUB-200-2011** | 2011 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://resolver.caltech.edu/CaltechCSTR:2010.001) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](http://www.vision.caltech.edu/datasets/cub_200_2011/) |
| **LAION-5B** | 2022 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2210.08402) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://laion.ai/blog/laion-5b/) |
| **DiffusionDB** | 2022 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2210.14896) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://poloclub.github.io/diffusiondb/) |

[<small>⇧ Back to ToC</small>](#contents)

---

## <span id="about-us">🎓 About Us</span>

QuenithAI is a professional organization composed of top researchers, dedicated to providing high-quality 1-on-1 research mentoring for university students worldwide. Our mission is to help students bridge the gap from theoretical knowledge to cutting-edge research and publish their work in top-tier conferences and journals.

Maintaining this `Awesome Text-to-Image Generation` list requires significant effort, just as completing a high-quality paper requires focused dedication and expert guidance. If you're looking for one-on-one support from top scholars on your own research project, to quickly identify innovative ideas and make publications, we invite you to contact us ASAP.

➡️ **Contact us via [WeChat](assets/wechat.jpg) or [E-mail](mailto:your.email@example.com) to start your research journey.**

---

「应达学术」(QuenithAI) 是一家由顶尖研究者组成，致力于为全球高校学生提供高质量1V1科研辅导的专业机构。我们的使命是帮助学生培养出色卓越的科研技能，在顶级会议和期刊上发表自己的成果。

维护一个GitHub调研仓库需要巨大的精力，正如完成一篇高质量的论文一样，离不开专注的投入和专业的指导。如果您希望在自己的研究项目中，获得来自顶尖学者的一对一支持，我们诚邀您与我们取得联系。

➡️ **欢迎通过 [微信](assets/wechat.jpg) 或 [邮件](mailto:your.email@example.com) 联系我们，开启您的科研之旅。**


[<small>⇧ Back to ToC</small>](#contents)

---



## <span id="contributing">🤝 Contributing</span>

Contributions are welcome! Please see our [**Contribution Guidelines**](CONTRIBUTING.md) for details on how to add new papers, correct information, or improve the repository.
