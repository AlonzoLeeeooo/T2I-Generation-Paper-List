<div align="center">
  <a href="YOUR_OFFICIAL_WEBSITE_URL">
    <img src="assets/logo_run_cn.png" alt="QuenithAI Logo" width="200" height="200">
  </a>
</div>

<div align="center">
  <h1>Awesome Text-to-Image Generation by QuenithAI</h1>
  <p>A curated collection of papers, models, and resources for the field of Text-to-Image Generation.</p>
  <p>
    <a href="https://awesome.re"><img src="https://awesome.re/badge.svg" alt="Awesome"></a>
    &nbsp;
    <a href="https://github.com/QuenithAI/T2I-Generation-Paper-List/pulls"><img src="https://img.shields.io/badge/PRs-Welcome-brightgreen.svg?style=flat-square" alt="PRs Welcome"></a>
    &nbsp;
    <a href="https://github.com/QuenithAI/T2I-Generation-Paper-List/issues"><img src="https://img.shields.io/badge/Issues-Welcome-orange?style=flat-square" alt="Issues Welcome"></a>
  </p>
</div>

> [!NOTE]
> This repository is proudly maintained by the frontline research mentors at **QuenithAI (应达学术)**. It aims to provide the most comprehensive and cutting-edge map of papers and technologies in the field of Text-to-Image generation.
>
> Your contributions are also vital—feel free to [open an issue](https://github.com/QuenithAI/T2I-Generation-Paper-List/issues) or [submit a pull request](https://github.com/QuenithAI/T2I-Generation-Paper-List/pulls) to become a collaborator of this repository. We expect your participation!
> 
>  If you require expert 1-on-1 guidance on your submissions to top-tier conferences and journals, we invite you to **contact us via [WeChat](assets/wechat.jpg) or [E-mail]((mailto:christzhaung@gmail.com))**.
>
>
> ---
>
> 本仓库由 **「应达学术」(QuenithAI)** 的一线科研导师团队倾力打造并持续维护，旨在为您呈现文生图领域最全面、最前沿的论文。
>
> 您的贡献对我们和社区来说至关重要——我们诚邀有志之士通过 [open an issue](https://github.com/QuenithAI/T2I-Generation-Paper-List/issues) 或 [submit a pull request](https://github.com/QuenithAI/T2I-Generation-Paper-List/pulls) 来成为这个项目的合作者之一，期待您的加入！
> 
> 如果您在冲刺科研顶会的道路上需要专业的1V1指导，欢迎**通过[微信](assets/wechat.jpg)或[邮件](mailto:christzhaung@gmail.com)联系我们**。


<details>
<summary><strong>⚡ Latest Updates</strong></summary>

- **(Aug 21th, 2025)**: Add a new direction: [🎨 Personalized Image Generation](#personalized).
- **(Aug 20th, 2025)**: Initial commit and repository structure established.

</details>

---

## <span id="contents">📚 Table of Contents</span>
- [📚 Table of Contents](#-table-of-contents)
- [📜 Papers \& Models](#-papers--models)
  - [✍️ Survey Papers](#️-survey-papers)
  - [🖼️ Text-to-Image Generation](#️-text-to-image-generation)
  - [🕹️ Conditional Image Generation](#️-conditional-image-generation)
  - [🎨 Personalized Image Generation](#-personalized-image-generation)
  - [✂️ Image Editing](#️-image-editing)
- [🗂️ Datasets](#️-datasets)
- [🎓 About Us](#-about-us)
- [🤝 Contributing](#-contributing)

---

## <span id="papers">📜 Papers & Models</span>

### <span id="survey">✍️ Survey Papers</span>



[<small>⇧ Back to ToC</small>](#contents)

### <span id="t2i">🖼️ Text-to-Image Generation</span>

<details>
<summary><h4>✨ 2025</h4></summary>

* **[CVPR 2025]** ***PreciseCam:*** *Precise Camera Control for Text-to-Image Generation*<br>
   [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2501.12910)
  [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://graphics.unizar.es/projects/PreciseCam2024/)
  [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/edurnebernal/PreciseCam)
  [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/edurnebb/PreciseCam)

* **[CVPR 2025]** ***Type‑R:*** *Automatically Retouching Typos for Text‑to‑Image Generation*<br>
   [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2411.18159) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/CyberAgentAILab/Type-R) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/cyberagent/type-r)

* **[CVPR 2025]** ***Compass Control:*** *Multi Object Orientation Control for Text‑to‑Image Generation*<br>
   [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2504.06752) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://rishubhpar.github.io/CompassControl/)

* **[CVPR 2025]** ***Generative Photography:*** *Scene‑Consistent Camera Control for Realistic Text‑to‑Image Synthesis*<br>
   [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.02168) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://generative-photography.github.io/project/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/pandayuanyu/generative-photography) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/pandaphd/generative_photography)

* **[CVPR 2025]** ***One‑Way Ticket:*** *Time‑Independent Unified Encoder for Distilling Text‑to‑Image Diffusion Models*<br>
   [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://cvpr.thecvf.com/virtual/2025/poster/32579) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/sen-mao/Loopfree) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/senmaonk/loopfree-sd2.1-base)

* **[CVPR 2025]** ***Text Embedding is Not All You Need:*** *Attention Control for Text‑to‑Image Semantic Alignment with Text Self‑Attention Maps*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.15236) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://t-sam-diffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/t-sam-diffusion/code)

* **[CVPR 2025]** ***Towards Uncertainty:*** *Understanding and Quantifying Uncertainty for Text‑to‑Image Generation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.03178) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ENSTA-U2IS-AI/Uncertainty_diffusion)

* **[CVPR 2025]** ***Responsible Diffusion:*** *Plug‑and‑Play Interpretable Responsible Text‑to‑Image Generation via Dual‑Space Multi‑faceted Concept Control*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.18324) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://basim-azam.github.io/responsiblediffusion/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/basim-azam/responsiblediffusion)

* **[CVPR 2025]** ***Make It Count:*** *Text‑to‑Image Generation with an Accurate Number of Objects*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2406.10210) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://make-it-count-paper.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Litalby1/make-it-count)

* **[CVPR 2025]** ***MCCD:*** *Multi‑Agent Collaboration‑based Compositional Diffusion for Complex Text‑to‑Image Generation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2505.02648)

* **[CVPR 2025]** ***Debias‑SD:*** *Rethinking Training for De‑biasing Text‑to‑Image Generation: Unlocking the Potential of Stable Diffusion*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.12692)

* **[CVPR 2025]** ***ShapeWords:*** *Guiding Text‑to‑Image Synthesis with 3D Shape‑Aware Prompts*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.02912) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://lodurality.github.io/shapewords/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lodurality/shapewords_paper_code) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/dmpetrov/shapewords)

* **[CVPR 2025]** ***SnapGen:*** *Taming High‑Resolution Text‑to‑Image Models for Mobile Devices with Efficient Architectures and Training*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.09619) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://snap-research.github.io/snapgen/)

* **[CVPR 2025]** ***STORM:*** *Spatial Transport Optimization by Repositioning Attention Map for Training‑Free Text‑to‑Image Synthesis*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.22168) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://micv-yonsei.github.io/storm2025/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/MICV-yonsei/STORM)

* **[CVPR 2025]** ***Focus‑N‑Fix:*** *Region‑Aware Fine‑Tuning for Text‑to‑Image Generation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2501.06481)

* **[CVPR 2025]** ***SILMM:*** *Self‑Improving Large Multimodal Models for Compositional Text‑to‑Image Generation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.05818) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://silmm.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LgQu/SILMM)

* **[CVPR 2025]** ***GLoCE:*** *Localized Concept Erasure for Text‑to‑Image Diffusion Models Using Training‑Free Gated Low‑Rank Adaptation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.12356) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://gl-oce.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Hyun1A/GLoCE)

* **[CVPR 2025]** ***Self‑Cross Guidance:*** *Self‑Cross Diffusion Guidance for Text‑to‑Image Synthesis of Similar Subjects*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.18936) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://selfcross-guidance.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mengtang-lab/selfcross-guidance)

* **[CVPR 2025]** ***Noise Diffusion:*** *Enhancing Semantic Faithfulness in Text‑to‑Image Synthesis*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.16503) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Bomingmiao/NoiseDiffusion)

* **[CVPR 2025]** ***PromptSampler:*** *Learning to Sample Effective and Diverse Prompts for Text‑to‑Image Generation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2410.07838) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/dbsxodud-11/PAG)

* **[CVPR 2025]** ***STEREO:*** *A Two‑Stage Framework for Adversarially Robust Concept Erasing from Text‑to‑Image Diffusion Models*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.16807) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/koushiksrivats/robust-concept-erasing)

* **[CVPR 2025]** ***MinorityPrompt:*** *Minority‑Focused Text‑to‑Image Generation via Prompt Optimization*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.16503) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/soobin-um/MinorityPrompt)

* **[CVPR 2025]** ***DistillT5:*** *Scaling Down Text Encoders of Text‑to‑Image Diffusion Models*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.19897) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LifuWang-66/DistillT5)

* **[CVPR 2025]** ***TIU:*** *The Illusion of Unlearning: The Unstable Nature of Machine Unlearning in Text‑to‑Image Diffusion Models*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/???) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/NGK2110/TIU)

* **[CVPR 2025]** ***Fuse‑DiT:*** *Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text‑to‑Image Synthesis*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/???) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/tang-bd/fuse-dit) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/ooutlierr/fuse-dit)

* **[CVPR 2025]** **Detect‑and‑Guide:** *Self‑regulation of Diffusion Models for Safe Text‑to‑Image Generation via Guideline Token Optimization*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.15197)

* **[CVPR 2025]** **Multi‑Group T2I:** *Multi‑Group Proportional Representations for Text‑to‑Image Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/papers/Jung_Multi-Group_Proportional_Representations_for_Text-to-Image_Models_CVPR_2025_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/sangwon-jung94/mpr-t2i)

* **[CVPR 2025]** **VODiff:** *Controlling Object Visibility Order in Text‑to‑Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/???) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/dliang293/VODiff)

* **[ICLR 2025]** **Improving Long‑Text Alignment:** *Improving Long‑Text Alignment for Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=2ZK8zyIt7o) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/luping-liu/LongAlign)

* **[ICLR 2025]** **ITTA:** *Information Theoretic Text‑to‑Image Alignment*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=Ugs2W5XFFo) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Chao0511/mitune)

* **[ICLR 2025]** **Meissonic:** *Revitalizing Masked Generative Transformers for Efficient High‑Resolution Text‑to‑Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=GJsuYHhAga) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sites.google.com/view/meissonic/home) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/viiika/Meissonic) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/MeissonFlow/Meissonic)

* **[ICLR 2025]** **PaRa:** *Personalizing Text‑to‑Image Diffusion via Parameter Rank Reduction*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=KZgo2YQbhc)

* **[ICLR 2025]** **Fluid:** *Scaling Autoregressive Text‑to‑image Generative Models with Continuous Tokens*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=jQP5o1VAVc)

* **[ICLR 2025]** **Prompt‑Pruning:** *Not All Prompts Are Made Equal – Prompt‑based Pruning of Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=3BhZCfJ73Y) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/rezashkv/diffusion_pruning) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/rezashkv/diffusion_pruning)

* **[ICLR 2025]** **Denoising AR Transformers:** *Denoising Autoregressive Transformers for Scalable Text‑to‑Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=amDkNPVWcn)

* **[ICLR 2025]** **Progressive Compositionality:** *Progressive Compositionality in Text‑to‑Image Generative Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=S85PP4xjFD) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://evansh666.github.io/EvoGen_Page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/evansh666/EvoGen)

* **[ICLR 2025]** **Classifier Scores:** *Mining your own secrets: Diffusion Classifier Scores for Continual Personalization of Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=hUdLs6TqZL) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://srvcodes.github.io/continual_personalization)

* **[ICLR 2025]** **Engagement:** *Measuring and Improving Engagement of Text‑to‑Image Generation Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=TmCcNuo03f) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://behavior-in-the-wild.github.io/image-engagement)

* **[ICLR 2025]** **Residual Gate Eraser:** *Concept Pinpoint Eraser for Text‑to-image Diffusion Models via Residual Attention Gate*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=ZRDhBwKs7l) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Hyun1A/CPE)

* **[ICLR 2025]** **Random Seeds:** *Enhancing Compositional Text‑to‑Image Generation with Reliable Random Seeds*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=5BSlakturs) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/doub7e/Reliable-Random-Seeds)

* **[ICLR 2025]** **One‑Prompt‑One‑Story:** *Free‑Lunch Consistent Text‑to‑Image Generation Using a Single Prompt*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=cD1kl2QKv1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://byliutao.github.io/1Prompt1Story.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/byliutao/1Prompt1Story)

* **[ICLR 2025]** **You Only Sample Once:** *Taming One‑Step Text‑to‑Image Synthesis by Self‑Cooperative Diffusion GANs*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=T7bmHkwzS6) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yoso-t2i.github.io) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Luo-Yihong/YOSO) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/Luo-Yihong/yoso_pixart512)

* **[ICLR 2025]** **Copyright Revisiting:** *Rethinking Artistic Copyright Infringements in the Era of Text‑to‑Image Generative Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=0OTVNEm9N4)

* **[ICLR 2025]** **Concept Combination Erasing:** *Erasing Concept Combination from Text‑to‑Image Diffusion Model*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=OBjF5I4PWg)

* **[ICLR 2025]** **Cross‑Attention Patterns:** *Cross‑Attention Head Position Patterns Can Align with Human Visual Concepts in Text‑to‑Image Generative Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=1vggIT5vvj) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/SNU-DRL/HRV)

* **[ICLR 2025]** **TIGeR:** *Unifying Text‑to‑Image Generation and Retrieval with Large Multimodal Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=mr2icR6dpD) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://tiger-t2i.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LgQu/TIGeR)

* **[ICLR 2025]** **DGQ:** *Distribution‑Aware Group Quantization for Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=ZyNEr7Xw5L) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ugonfor/DGQ)

* **[ICLR 2025]** **Jacobi Decoding:** *Accelerating Auto‑regressive Text‑to‑Image Generation with Training‑free Speculative Jacobi Decoding*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=LZfjxvqw0N) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/tyshiwo1/Accelerating-T2I-AR-with-SJD)

* **[ICLR 2025]** **PT‑T2I/V:** *An Efficient Proxy‑Tokenized Diffusion Transformer for Text‑to‑Image/Video Task*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=lTrrnNdkOX) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://360cvgroup.github.io/Qihoo-T2X/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/360CVGroup/Qihoo-T2X)

* **[ICLR 2025]** **Gecko Evaluation:** *Revisiting Text‑to‑Image Evaluation with Gecko: on Metrics, Prompts, and Human Rating*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=Im2neAMlre) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/google-deepmind/gecko_benchmark_t2i)

* **[ICLR 2025]** **SANA:** *Efficient High‑Resolution Text‑to‑Image Synthesis with Linear Diffusion Transformers*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=N8Oj1XhtYZ) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://nvlabs.github.io/Sana) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/NVlabs/Sana) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/Efficient-Large-Model/Sana_1600M_1024px_diffusers)

* **[ICLR 2025]** **Rectified Flow:** *Text‑to‑Image Rectified Flow as Plug‑and‑Play Priors*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=SzPZK856iI) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/yangxiaofeng/rectified_flow_prior)

* **[ICLR 2025]** **Human Feedback Filtering:** *Automated Filtering of Human Feedback Data for Aligning Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=8jvVNPHtVJ)

* **[ICLR 2025]** **SAFREE:** *Training‑Free and Adaptive Guard for Safe Text‑to‑Image and Video Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=hgTFotBRKl) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://safree-safe-t2i-t2v.github.io) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/jaehong31/SAFREE)

* **[ICLR 2025]** **IterComp:** *Iterative Composition‑Aware Feedback Learning from Model Gallery for Text‑to‑Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=4w99NAikOE) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/YangLing0818/IterComp) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/comin/IterComp)

* **[ICLR 2025]** **ScImage:** *How good are multimodal large language models at scientific text‑to‑image generation?*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=ugyqNEOjoU) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/casszhao/ScImage)

* **[ICLR 2025]** **Score Distillation:** *Guided Score Identity Distillation for Data‑Free One‑Step Text‑to‑Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=HMVDiaWMwM) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mingyuanzhou/SiD-LSG) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/UT-Austin-PML/SiD-LSG)

* **[ICLR 2025]** **Causal Variation:** *Evaluating Semantic Variation in Text‑to‑Image Synthesis: A Causal Perspective*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=NWb128pSCb) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zhuxiangru/SemVarBench)


<details>
<summary><h4>✅ Published Papers</h4></summary>

</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>✨ 2024</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2024]** ***DistriFusion:*** *Distributed Parallel Inference for High-Resolution Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.19481.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mit-han-lab/distrifuser)

* **[CVPR 2024]** ***InstanceDiffusion:*** *Instance-level Control for Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.03290.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://people.eecs.berkeley.edu/~xdwang/projects/InstDiff/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/frank-xwang/InstanceDiffusion)

* **[CVPR 2024]** ***ECLIPSE:*** *A Resource-Efficient Text-to-Image Prior for Image Generations*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.04655.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://github.com/eclipse-t2i/eclipse-inference) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://eclipse-t2i.vercel.app/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/ECLIPSE-Community/ECLIPSE-Kandinsky-v2.2)

* **[CVPR 2024]** ***Instruct-Imagen:*** *Image Generation with Multi-modal Instruction*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2401.01952.pdf)

* **[CVPR 2024]** ***Continuous 3D Words:*** *Learning Continuous 3D Words for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.08654.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ttchengab/continuous_3d_words_code/)

* **[CVPR 2024]** ***HanDiffuser:*** *Text-to-Image Generation With Realistic Hand Appearances*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.01693.pdf)

* **[CVPR 2024]** ***Rich Human Feedback:*** *Rich Human Feedback for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.10240.pdf)

* **[CVPR 2024]** ***MarkovGen:*** *Structured Prediction for Efficient Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2308.10997.pdf)

* **[CVPR 2024]** ***Customization Assistant:*** *Customization Assistant for Text-to-image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.03045.pdf)

* **[CVPR 2024]** ***ADI:*** *Learning Disentangled Identifiers for Action-Customized Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.15841.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://adi-t2i.github.io/ADI/)

* **[CVPR 2024]** ***UFOGen:*** *You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.09257.pdf)

* **[CVPR 2024]** ***Interpret Diffusion:*** *Self-Discovering Interpretable Diffusion Latent Directions for Responsible Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.17216.pdf)

* **[CVPR 2024]** ***Tailored Visions:*** *Enhancing Text-to-Image Generation with Personalized Prompt Rewriting*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.08129.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zzjchen/Tailored-Visions)

* **[CVPR 2024]** ***CoDi:*** *Conditional Diffusion Distillation for Higher-Fidelity and Faster Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.01407.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://fast-codi.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/fast-codi/CoDi) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/MKFMIKU/CoDi)

* **[CVPR 2024]** ***Arbitrary‑Scale Diffusion:*** *Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion Model and Implicit Neural Decoder*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.10255.pdf)

* **[CVPR 2024]** ***Human-Centric Priors:*** *Towards Effective Usage of Human-Centric Priors in Diffusion Models for Text-based Human Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.05239)

* **[CVPR 2024]** ***ElasticDiffusion:*** *Training-free Arbitrary Size Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.18822) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://elasticdiffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/MoayedHajiAli/ElasticDiffusion-official)

* **[CVPR 2024]** ***CosmicMan:*** *A Text-to-Image Foundation Model for Humans*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.01294) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://cosmicman-cvpr2024.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/cosmicman-cvpr2024/CosmicMan)

* **[CVPR 2024]** ***PanFusion:*** *Taming Stable Diffusion for Text to 360° Panorama Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.07949) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://chengzhag.github.io/publication/panfusion) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/chengzhag/PanFusion)

* **[CVPR 2024]** ***Intelligent Grimm:*** *Open-ended Visual Storytelling via Latent Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2306.00973) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://haoningwu3639.github.io/StoryGen_Webpage/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/haoningwu3639/StoryGen)

* **[CVPR 2024]** ***Scalability:*** *On the Scalability of Diffusion-based Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.02883)

* **[CVPR 2024]** ***MuLAn:*** *A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.02790) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://mulan-dataset.github.io/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/mulan-dataset/v1.0)

* **[CVPR 2024]** ***Multi-dimensional Preferences:*** *Learning Multi-dimensional Human Preference for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2405.14705)

* **[CVPR 2024]** ***Dynamic Prompts:*** *Dynamic Prompt Optimizing for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.04095)

* **[CVPR 2024]** ***Reinforcement Diversification:*** *Training Diffusion Models Towards Diverse Image Generation with Reinforcement Learning*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Miao_Training_Diffusion_Models_Towards_Diverse_Image_Generation_with_Reinforcement_Learning_CVPR_2024_paper.pdf)

* **[CVPR 2024]** ***HypercGAN:*** *Adversarial Text to Continuous Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Haydarov_Adversarial_Text_to_Continuous_Image_Generation_CVPR_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://kilichbek.github.io/webpage/hypercgan/)

* **[CVPR 2024]** ***EmoGen:*** *Emotional Image Content Generation with Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_EmoGen_Emotional_Image_Content_Generation_with_Text-to-Image_Diffusion_Models_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/JingyuanYY/EmoGen)

* **[ECCV 2024]** ***LaVi‑Bridge:*** *Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.07860) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://shihaozhaozsh.github.io/LaVi-Bridge/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ShihaoZhaoZSH/LaVi-Bridge)

* **[ECCV 2024]** ***DiffPNG:*** *Exploring Phrase-Level Grounding with Text-to-Image Diffusion Model*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2407.05352v1) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/nini0919/DiffPNG)

* **[ECCV 2024]** ***SPRIGHT:*** *Getting it Right: Improving Spatial Consistency in Text-to-Image Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.01197) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://spright-t2i.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/SPRIGHT-T2I/SPRIGHT)

* **[ECCV 2024]** ***IndicTTI:*** *Navigating Text-to-Image Generative Bias across Indic Languages*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.00283v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://iab-rubric.org/resources/other-databases/indictti)

* **[ECCV 2024]** ***Safeguard T2I:*** *Safeguard Text-to-Image Diffusion Models with Human Feedback Inversion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2407.21032)

* **[ECCV 2024]** ***Reality-and-Fantasy:*** *The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2407.12579) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://leo81005.github.io/Reality-and-Fantasy/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://leo81005.github.io/Reality-and-Fantasy/)

* **[ECCV 2024]** ***RECE:*** *Reliable and Efficient Concept Erasure of Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2407.12383v1) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/CharlesGong12/RECE)

* **[ECCV 2024]** ***StyleTokenizer:*** *Defining Image Style by a Single Instance for Controlling Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2409.02543) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/alipay/style-tokenizer)

* **[ECCV 2024]** ***PEA-Diffusion:*** *Parameter-Efficient Adapter with Knowledge Distillation in non-English Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08492.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/OPPO-Mente-Lab/PEA-Diffusion)

* **[ECCV 2024]** ***Skewed Relations T2I:*** *Skews in the Phenomenon Space Hinder Generalization in Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11936.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zdxdsw/skewed_relations_T2I)

* **[ECCV 2024]** ***Parrot:*** *Pareto-optimal Multi-Reward Reinforcement Learning Framework for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05562.pdf)

* **[ECCV 2024]** ***MobileDiffusion:*** *Instant Text-to-Image Generation on Mobile Devices*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07923.pdf)

* **[ECCV 2024]** ***PixArt-Σ:*** *Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.04692) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pixart-alpha.github.io/PixArt-sigma-project/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/PixArt-alpha/PixArt-sigma)

* **[ECCV 2024]** ***CogView3:*** *Finer and Faster Text-to-Image Generation via Relay Diffusion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.05121) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/THUDM/CogView)

* **[ICLR 2024]** ***Patched Diffusion Models:*** *Patched Denoising Diffusion Models For High-Resolution Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2308.01316.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mlpc-ucsd/patch-dm)

* **[ICLR 2024]** ***Relay Diffusion:*** *Unifying diffusion process across resolutions for image synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2309.03350.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/THUDM/RelayDiffusion)

* **[ICLR 2024]** ***SDXL:*** *Improving Latent Diffusion Models for High-Resolution Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2307.01952.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Stability-AI/generative-models)

* **[ICLR 2024]** ***Compose and Conquer:*** *Diffusion-Based 3D Depth Aware Composable Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2401.09048.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/tomtom1103/compose-and-conquer)

* **[ICLR 2024]** ***PixArt-α:*** *Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.00426.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pixart-alpha.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/PixArt-alpha/PixArt-alpha) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/PixArt-alpha/PixArt-alpha)

* **[SIGGRAPH 2024]** ***RGB↔X:*** *Image Decomposition and Synthesis Using Material- and Lighting-aware Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://zheng95z.github.io/assets/files/sig24-rgbx.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zheng95z.github.io/publications/rgbx24)

* **[AAAI 2024]** ***Semantic-aware Augmentation:*** *Semantic-aware Data Augmentation for Text-to-image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.07951.pdf)

* **[AAAI 2024]** ***Abstract Concepts:*** *Text-to-Image Generation for Abstract Concepts*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://ojs.aaai.org/index.php/AAAI/article/view/28122)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>✨ 2023</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2023]** ***GigaGAN:*** *Scaling Up GANs for Text-to-Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Scaling_Up_GANs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://mingukkang.github.io/GigaGAN/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lucidrains/gigagan-pytorch)

* **[CVPR 2023]** ***ERNIE-ViLG 2.0:*** *Improving Text-to-Image Diffusion Model With Knowledge-Enhanced Mixture-of-Denoising-Experts*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_ERNIE-ViLG_2.0_Improving_Text-to-Image_Diffusion_Model_With_Knowledge-Enhanced_Mixture-of-Denoising-Experts_CVPR_2023_paper.pdf)

* **[CVPR 2023]** ***Shifted Diffusion:*** *Shifted Diffusion for Text-to-image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Shifted_Diffusion_for_Text-to-Image_Generation_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/drboog/Shifted_Diffusion)

* **[CVPR 2023]** ***GALIP:*** *Generative Adversarial CLIPs for Text-to-Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tao_GALIP_Generative_Adversarial_CLIPs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/tobran/GALIP)

* **[CVPR 2023]** ***Specialist Diffusion:*** *Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image Diffusion Models to Learn Any Unseen Style*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Specialist_Diffusion_Plug-and-Play_Sample-Efficient_Fine-Tuning_of_Text-to-Image_Diffusion_Models_To_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Picsart-AI-Research/Specialist-Diffusion)

* **[CVPR 2023]** ***Verifiable Evaluation:*** *Toward Verifiable and Reproducible Human Evaluation for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Otani_Toward_Verifiable_and_Reproducible_Human_Evaluation_for_Text-to-Image_Generation_CVPR_2023_paper.pdf)

* **[CVPR 2023]** ***RIATIG:*** *Reliable and Imperceptible Adversarial Text-to-Image Generation with Natural Prompts*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_RIATIG_Reliable_and_Imperceptible_Adversarial_Text-to-Image_Generation_With_Natural_Prompts_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/WUSTL-CSPL/RIATIG)

* **[CVPR 2023]** ***Custom Diffusion:*** *Multi-Concept Customization of Text-to-Image Diffusion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kumari_Multi-Concept_Customization_of_Text-to-Image_Diffusion_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://www.cs.cmu.edu/~custom-diffusion/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/adobe-research/custom-diffusion)

* **[ICCV 2023]** ***DiffFit:*** *Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_DiffFit_Unlocking_Transferability_of_Large_Diffusion_Models_via_Simple_Parameter-efficient_ICCV_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mkshing/DiffFit-pytorch)

* **[NeurIPS 2023]** ***ImageReward:*** *Learning and Evaluating Human Preferences for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=JVzeOYEx6d) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/THUDM/ImageReward)

* **[NeurIPS 2023]** ***RAPHAEL:*** *Text-to-Image Generation via Large Mixture of Diffusion Paths*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.18295) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://raphael-painter.github.io/)

* **[NeurIPS 2023]** ***Linguistic Binding:*** *Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=AOKU4nRw1W) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/RoyiRa/Linguistic-Binding-in-Diffusion-Models)

* **[NeurIPS 2023]** ***DenseDiffusion:*** *Dense Text-to-Image Generation with Attention Modulation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Dense_Text-to-Image_Generation_with_Attention_Modulation_ICCV_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/naver-ai/densediffusion)

* **[ICLR 2023]** ***Structured Diffusion Guidance:*** *Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=PUIqjT4rzq7) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/weixi-feng/Structured-Diffusion-Guidance)

* **[ICML 2023]** ***StyleGAN-T:*** *Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.mlr.press/v202/sauer23a/sauer23a.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sites.google.com/view/stylegan-t/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/autonomousvision/stylegan-t)

* **[ICML 2023]** ***Muse:*** *Text-To-Image Generation via Masked Generative Transformers*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.mlr.press/v202/chang23b/chang23b.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://muse-icml.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lucidrains/muse-maskgit-pytorch)

* **[ICML 2023]** ***UniDiffusers:*** *One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2303.06555) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/thu-ml/unidiffuser)

* **[ACM MM 2023]** ***SUR-adapter:*** *Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.05189.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Qrange-group/SUR-adapter)

* **[ACM MM 2023]** ***ControlStyle:*** *Text-Driven Stylized Image Generation Using Diffusion Priors*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.05463.pdf)

* **[SIGGRAPH 2023]** ***Attend-and-Excite:*** *Attention-Based Semantic Guidance for Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2301.13826.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yuval-alaluf.github.io/Attend-and-Excite/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/yuval-alaluf/Attend-and-Excite) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/AttendAndExcite/Attend-and-Excite)



</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

[<small>⇧ Back to ToC</small>](#contents)

### <span id="conditional">🕹️ Conditional Image Generation</span>

<details>
<summary><h4>✨ 2025</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2024]** ***PLACE:*** *Adaptive Layout‑Semantic Fusion for Semantic Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.01852.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/cszy98/PLACE)

* **[CVPR 2024]** ***One‑Shot Structure‑Aware Stylized Image Synthesis:*** *One‑Shot Structure‑Aware Stylized Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.17275.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/hansam95/OSASIS)

* **[CVPR 2024]** ***Attention Refocusing:*** *Grounded Text‑to‑Image Synthesis with Attention Refocusing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2306.05427.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://attention-refocusing.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Attention-Refocusing/attention-refocusing) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/attention-refocusing/Attention-refocusing)

* **[CVPR 2024]** ***CFLD:*** *Coarse‑to‑Fine Latent Diffusion for Pose‑Guided Person Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.18078.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/YanzuoLu/CFLD)

* **[CVPR 2024]** ***DetDiffusion:*** *Synergizing Generative and Perceptive Models for Enhanced Data Generation and Perception*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.13304)

* **[CVPR 2024]** ***CAN:*** *Condition‑Aware Neural Network for Controlled Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.01143.pdf)

* **[CVPR 2024]** ***SceneDiffusion:*** *Move Anything with Layered Scene Diffusion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.07178)

* **[CVPR 2024]** ***Zero‑Painter:*** *Training‑Free Layout Control for Text‑to‑Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Ohanyan_Zero-Painter_Training-Free_Layout_Control_for_Text-to-Image_Synthesis_CVPR_2024_paper.pdf) 

* **[CVPR 2024]** ***MIGC:*** *Multi‑Instance Generation Controller for Text‑to‑Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_MIGC_Multi-Instance_Generation_Controller_for_Text-to-Image_Synthesis_CVPR_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://migcproject.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/limuloo/MIGC)

* **[CVPR 2024]** ***FreeControl:*** *Training‑Free Spatial Control of Any Text‑to‑Image Diffusion Model with Any Condition*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Mo_FreeControl_Training-Free_Spatial_Control_of_Any_Text-to-Image_Diffusion_Model_with_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/genforce/freecontrol)

* **[ECCV 2024]** ***PreciseControl:*** *Enhancing Text‑To‑Image Diffusion Models with Fine‑Grained Attribute Control*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.05083) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://rishubhpar.github.io/PreciseControl.home/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/rishubhpar/PreciseControl)

* **[ECCV 2024]** ***AnyControl:*** *Create Your Artwork with Versatile Control on Text‑to‑Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01706.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/open-mmlab/AnyControl)

* **[NeurIPS 2024]** ***Ctrl‑X:*** *Controlling Structure and Appearance for Text‑To‑Image Generation Without Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2406.07540) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://genforce.github.io/ctrl-x/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/genforce/ctrl-x)

* **[ICLR 2024]** ***PCDMs:*** *Advancing Pose‑Guided Image Synthesis with Progressive Conditional Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.06313.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/muzishen/PCDMs)

* **[WACV 2024]** ***Layout Control with Cross‑Attention Guidance:*** *Training‑Free Layout Control with Cross‑Attention Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Training-Free_Layout_Control_With_Cross-Attention_Guidance_WACV_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://silent-chen.github.io/layout-guidance/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/silent-chen/layout-guidance) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/silentchen/layout-guidance)

* **[AAAI 2024]** ***SSMG:*** *Spatial‑Semantic Map Guided Diffusion Model for Free‑form Layout‑to‑image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2308.10156.pdf)

* **[AAAI 2024]** ***Attention Map Control:*** *Compositional Text‑to‑Image Synthesis with Attention Map Control of Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.13921.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/OPPO-Mente-Lab/attention-mask-control)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>✨ 2024</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2024]** ***PLACE:*** *Adaptive Layout‑Semantic Fusion for Semantic Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.01852.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/cszy98/PLACE)

* **[CVPR 2024]** ***One‑Shot Structure‑Aware Stylized Image Synthesis:*** *One‑Shot Structure‑Aware Stylized Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.17275.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/hansam95/OSASIS)

* **[CVPR 2024]** ***Attention Refocusing:*** *Grounded Text‑to‑Image Synthesis with Attention Refocusing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2306.05427.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://attention-refocusing.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Attention-Refocusing/attention-refocusing) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/attention-refocusing/Attention-refocusing)

* **[CVPR 2024]** ***CFLD:*** *Coarse‑to‑Fine Latent Diffusion for Pose‑Guided Person Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.18078.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/YanzuoLu/CFLD)

* **[CVPR 2024]** ***DetDiffusion:*** *Synergizing Generative and Perceptive Models for Enhanced Data Generation and Perception*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.13304) 

* **[CVPR 2024]** ***CAN:*** *Condition‑Aware Neural Network for Controlled Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.01143.pdf)

* **[CVPR 2024]** ***SceneDiffusion:*** *Move Anything with Layered Scene Diffusion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.07178)

* **[CVPR 2024]** ***Zero‑Painter:*** *Training‑Free Layout Control for Text‑to‑Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Ohanyan_Zero-Painter_Training-Free_Layout_Control_for_Text-to-Image_Synthesis_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Picsart-AI-Research/Zero-Painter)

* **[CVPR 2024]** ***MIGC:*** *Multi‑Instance Generation Controller for Text‑to‑Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_MIGC_Multi-Instance_Generation_Controller_for_Text-to-Image_Synthesis_CVPR_2024_paper.pdf) [![Project Page](https://img.shields.io-badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://migcproject.github.io/) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/limuloo/MIGC)

* **[CVPR 2024]** ***FreeControl:*** *Training‑Free Spatial Control of Any Text‑to‑Image Diffusion Model with Any Condition*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Mo_FreeControl_Training-Free_Spatial_Control_of_Any_Text-to-Image_Diffusion_Model_with_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/genforce/freecontrol)

* **[ECCV 2024]** ***PreciseControl:*** *Enhancing Text‑To‑Image Diffusion Models with Fine‑Grained Attribute Control*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.05083) [![Project Page](https://img.shields.io-badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://rishubhpar.github.io/PreciseControl.home/) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/rishubhpar/PreciseControl) 

* **[ECCV 2024]** ***AnyControl:*** *Create Your Artwork with Versatile Control on Text‑to‑Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01706.pdf) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/open-mmlab/AnyControl) 

* **[NeurIPS 2024]** ***Ctrl‑X:*** *Controlling Structure and Appearance for Text‑To‑Image Generation Without Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2406.07540) [![Project Page](https://img.shields.io-badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://genforce.github.io/ctrl-x/) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/genforce/ctrl-x) 

* **[ICLR 2024]** ***PCDMs:*** *Advancing Pose‑Guided Image Synthesis with Progressive Conditional Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.06313.pdf) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/muzishen/PCDMs) 

* **[WACV 2024]** ***Layout Control with Cross‑Attention Guidance:*** *Training‑Free Layout Control with Cross‑Attention Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Training-Free_Layout_Control_With_Cross-Attention_Guidance_WACV_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://silent-chen.github.io/layout-guidance/) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/silent-chen/layout-guidance) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/silentchen/layout-guidance)

* **[AAAI 2024]** ***SSMG:*** *Spatial‑Semantic Map Guided Diffusion Model for Free‑form Layout‑to‑image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2308.10156.pdf)

* **[AAAI 2024]** ***Attention Map Control:*** *Compositional Text‑to‑Image Synthesis with Attention Map Control of Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.13921.pdf) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/OPPO-Mente-Lab/attention-mask-control)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>✨ 2023</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2023]** ***GLIGEN:*** *Open-Set Grounded Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_GLIGEN_Open-Set_Grounded_Text-to-Image_Generation_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://gligen.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/gligen/GLIGEN) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/gligen/demo)

* **[CVPR 2022]** ***Autoregressive Image Generation:*** *Using Residual Quantization*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2022/papers/Lee_Autoregressive_Image_Generation_Using_Residual_Quantization_CVPR_2022_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/kakaobrain/rq-vae-transformer)

* **[CVPR 2023]** ***SpaText:*** *Spatio-Textual Representation for Controllable Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Avrahami_SpaText_Spatio-Textual_Representation_for_Controllable_Image_Generation_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://omriavrahami.com/spatext/)

* **[CVPR 2022]** ***Text to Image Generation with Semantic-Spatial Aware GAN:*** *Text to Image Generation with Semantic-Spatial Aware GAN*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2022/papers/Liao_Text_to_Image_Generation_With_Semantic-Spatial_Aware_GAN_CVPR_2022_paper.pdf)

* **[CVPR 2023]** ***ReCo:*** *Region-Controlled Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_ReCo_Region-Controlled_Text-to-Image_Generation_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/microsoft/ReCo)

* **[CVPR 2023]** ***LayoutDiffusion:*** *Controllable Diffusion Model for Layout-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_LayoutDiffusion_Controllable_Diffusion_Model_for_Layout-to-Image_Generation_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ZGCTroy/LayoutDiffusion)

* **[ICLR 2023]** ***Ctrl-U:*** *Robust Conditional Image Generation via Uncertainty-aware Reward Modeling*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=eC2ICbECNM) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://grenoble-zhang.github.io/Ctrl-U-Page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/grenoble-zhang/Ctrl-U)

* **[ICCV 2023]** ***ControlNet:*** *Adding Conditional Control to Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Adding_Conditional_Control_to_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lllyasviel/ControlNet) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/lllyasviel/ControlNet)

* **[ICCV 2023]** ***SceneGenie:*** *Scene Graph Guided Diffusion Models for Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023W/SG2RL/papers/Farshad_SceneGenie_Scene_Graph_Guided_Diffusion_Models_for_Image_Synthesis_ICCVW_2023_paper.pdf)

* **[ICCV 2023]** ***ZestGuide:*** *Zero-Shot Spatial Layout Conditioning for Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Couairon_Zero-Shot_Spatial_Layout_Conditioning_for_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf)

* **[ICML 2023]** ***Composer:*** *Creative and Controllable Image Synthesis with Composable Conditions*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.mlr.press/v202/huang23b/huang23b.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ali-vilab.github.io/composer-page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ali-vilab/composer)

* **[ICML 2023]** ***MultiDiffusion:*** *Fusing Diffusion Paths for Controlled Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.mlr.press/v202/bar-tal23a/bar-tal23a.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://multidiffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/omerbt/MultiDiffusion) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/weizmannscience/MultiDiffusion)

* **[SIGGRAPH 2023]** ***Sketch-Guided Text-to-Image Diffusion Models:*** *Sketch-Guided Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://dl.acm.org/doi/pdf/10.1145/3588432.3591560) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sketch-guided-diffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ogkalu2/Sketch-Guided-Stable-Diffusion)

* **[NeurIPS 2023]** ***Uni-ControlNet:*** *All-in-One Control to Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.16322.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://shihaozhaozsh.github.io/unicontrolnet/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ShihaoZhaoZSH/Uni-ControlNet)

* **[NeurIPS 2023]** ***Prompt Diffusion:*** *In-Context Learning Unlocked for Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=6BZS2EAkns) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zhendong-wang.github.io/prompt-diffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Zhendong-Wang/Prompt-Diffusion)

* **[WACV 2023]** ***More Control for Free!:*** *Image Synthesis with Semantic Diffusion Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/WACV2023/papers/Liu_More_Control_for_Free_Image_Synthesis_With_Semantic_Diffusion_Guidance_WACV_2023_paper.pdf)

* **[ACM MM 2023]** ***LayoutLLM-T2I:*** *Eliciting Layout Guidance from LLM for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2308.05095.pdf)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

[<small>⇧ Back to ToC</small>](#contents)

### <span id="personalized">🎨 Personalized Image Generation</span>

<details>
<summary><h4>✨ 2025</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2025]** ***SerialGen:*** *Personalized Image Generation by First Standardization Then Personalization*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.01485) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://serialgen.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/)

* **[CVPR 2025]** ***PatchDPO:*** *Patch-level DPO for Finetuning-free Personalized Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.03177) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://github.com/hqhQAQ/PatchDPO) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/hqhQAQ/PatchDPO) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/hqhQAQ/PatchDPO)

* **[CVPR 2025]** ***DreamCache:*** *Finetuning-Free Lightweight Personalized Image Generation via Feature Caching*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.17786) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://emanuele97x.github.io/DreamCache) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Emanuele97x/DreamCache)

* **[NeurIPS 2025]** ***MS-Diffusion:*** *Multi-Subject Zero-shot Image Personalization with Layout Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2406.07209) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ms-diffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/MS-Diffusion/MS-Diffusion) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/doge1516/MS-Diffusion)

* **[NeurIPS 2025]** ***ClassDiffusion:*** *More Aligned Personalization Tuning with Explicit Class Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=iTm4H6N4aG) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://classdiffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Rbrq03/ClassDiffusion)

* **[NeurIPS 2025]** ***DreamBench++:*** *A Human-Aligned Benchmark for Personalized Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2406.16855) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://dreambenchplus.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/yuangpeng/dreambench_plus) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/yuangpeng/dreambench_plus)

* **[NeurIPS 2025]** ***TweedieMix:*** *Improving Multi-Concept Fusion for Diffusion-based Image/Video Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2410.05591) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://github.com/KwonGihyun/TweedieMix) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/KwonGihyun/TweedieMix)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>✨ 2024</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2024]** ***Cross Initialization:*** *Personalized Text‑to‑Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.15905.pdf)

* **[CVPR 2024]** ***When StyleGAN Meets Stable Diffusion:*** *a W+ Adapter for Personalized Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.17461.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://csxmli2016.github.io/projects/w-plus-adapter/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/csxmli2016/w-plus-adapter)

* **[CVPR 2024]** ***Style Aligned:*** *Image Generation via Shared Attention*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.02133.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://style-aligned-gen.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/google/style-aligned)

* **[CVPR 2024]** ***InstantBooth:*** *Personalized Text‑to‑Image Generation without Test‑Time Finetuning*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2304.03411.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://jshi31.github.io/InstantBooth/)

* **[CVPR 2024]** ***High Fidelity:*** *Person‑centric Subject‑to‑Image Synthesis*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.10329.pdf)

* **[CVPR 2024]** ***RealCustom:*** *Narrowing Real Text Word for Real‑Time Open‑Domain Text‑to‑Image Customization*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.00483.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://corleone-huang.github.io/realcustom/) [![🤗 Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/bytedance-research/RealCustom)

* **[CVPR 2024]** ***DisenDiff:*** *Attention Calibration for Disentangled Text‑to‑Image Personalization*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.18551) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Monalissaa/DisenDiff)

* **[CVPR 2024]** ***FreeCustom:*** *Tuning‑Free Customized Image Generation for Multi‑Concept Composition*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2405.13870v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://aim-uofa.github.io/FreeCustom/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/aim-uofa/FreeCustom)

* **[CVPR 2024]** ***Personalized Residuals:*** *for Concept‑Driven Text‑to‑Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2405.12978)

* **[CVPR 2024]** ***Subject‑Agnostic Guidance:*** *Improving Subject‑Driven Image Synthesis*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2405.01356)

* **[CVPR 2024]** ***JeDi:*** *Joint‑Image Diffusion Models for Finetuning‑Free Personalized Text‑to‑Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zeng_JeDi_Joint-Image_Diffusion_Models_for_Finetuning-Free_Personalized_Text-to-Image_Generation_CVPR_2024_paper.pdf)

* **[CVPR 2024]** ***Influence Watermarks:*** *Countering Personalized Text‑to‑Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Countering_Personalized_Text-to-Image_Generation_with_Influence_Watermarks_CVPR_2024_paper.pdf)

* **[CVPR 2024]** ***PIA:*** *Your Personalized Image Animator via Plug‑and‑Play Modules in Text‑to‑Image Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_PIA_Your_Personalized_Image_Animator_via_Plug-and-Play_Modules_in_Text-to-Image_CVPR_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pi-animator.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/open-mmlab/PIA)

* **[CVPR 2024]** ***SSR‑Encoder:*** *Encoding Selective Subject Representation for Subject‑Driven Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_SSR-Encoder_Encoding_Selective_Subject_Representation_for_Subject-Driven_Generation_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Xiaojiu-z/SSR_Encoder)

* **[ECCV 2024]** ***Be Yourself:*** *Bounded Attention for Multi‑Subject Text‑to‑Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.16990) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://omer11a.github.io/bounded-attention/)

* **[ECCV 2024]** ***Powerful and Flexible:*** *Personalized Text‑to‑Image Generation via Reinforcement Learning*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://arxiv.org/pdf/2407.06642v1) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/wfanyue/DPG-T2I-Personalization)

* **[ECCV 2024]** ***TIGC:*** *Tuning‑Free Image Customization with Image and Text Guidance*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.12658) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zrealli.github.io/TIGIC/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zrealli/TIGIC)

* **[ECCV 2024]** ***MasterWeaver:*** *Taming Editability and Face Identity for Personalized Text‑to‑Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06786.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://masterweaver.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/csyxwei/MasterWeaver)

* **[NeurIPS 2024]** ***RectifID:*** *Personalizing Rectified Flow with Anchored Classifier Guidance*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.neurips.cc/paper_files/paper/2024/file/afa58a5b6adc0845e0fd632132a64c39-Paper-Conference.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/feifeiobama/RectifID)

* **[NeurIPS 2024]** ***AttnDreamBooth:*** *Towards Text‑Aligned Personalized Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.neurips.cc/paper_files/paper/2024/file/465a13a95741fab2e912f98adb07df1d-Paper-Conference.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://attndreambooth.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lyuPang/AttnDreamBooth)

* **[AAAI 2024]** ***Decoupled Textual Embeddings:*** *for Customized Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.11826.pdf)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>✨ 2023</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2023]** ***Custom Diffusion:*** *Multi-Concept Customization of Text-to-Image Diffusion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kumari_Multi-Concept_Customization_of_Text-to-Image_Diffusion_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://www.cs.cmu.edu/~custom-diffusion/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/adobe-research/custom-diffusion) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/custom-diffusion-library/cat)

* **[CVPR 2023]** ***DreamBooth:*** *Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://dreambooth.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/google/dreambooth) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/BAAI/DreamBooth-AltDiffusion)

* **[ICCV 2023]** ***ELITE:*** *Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_ELITE_Encoding_Visual_Concepts_into_Textual_Embeddings_for_Customized_Text-to-Image_ICCV_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://github.com/csyxwei/ELITE) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/csyxwei/ELITE) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/ELITE-library/ELITE)

* **[ICLR 2023]** ***Textual Inversion:*** *An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=NAQvF08TcyG) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://textual-inversion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/rinongal/textual_inversion) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/Clyuue/textual_inversion_cat)

* **[SIGGRAPH Asia 2023]** ***Break-A-Scene:*** *Extracting Multiple Concepts from a Single Image*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.16311.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://omriavrahami.com/break-a-scene) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/google/break-a-scene)

* **[SIGGRAPH 2023]** ***Encoder‑Based Domain Tuning:*** *Encoder‑Based Domain Tuning for Fast Personalization of Text‑to‑Image Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2302.12228.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://tuning-encoder.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mkshing/e4t-diffusion)

* **[SIGGRAPH 2023]** ***LayerDiffusion:*** *Layered Controlled Image Editing with Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://dl.acm.org/doi/pdf/10.1145/3610543.3626172) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zrealli.github.io/layerdiffusion/index.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lllyasviel/LayerDiffuse) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/LayerDiffusion/layerdiffusion-v1)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

[<small>⇧ Back to ToC</small>](#contents)

### <span id="editing">✂️ Image Editing</span>

<details>
<summary><h4>✨ 2025</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2025]** ***FDS:*** *Frequency‑Aware Denoising Score for Text‑Guided Latent Diffusion Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.19191) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ivrl.github.io/fds-webpage/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/IVRL/FDS)


* **[CVPR 2025]** *Reference‑Based 3D‑Aware Image Editing with Triplanes*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.03632) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://three-bee.github.io/triplane_edit/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/three-bee/triplane_edit)


* **[CVPR 2025]** ***MoEdit:*** *On Learning Quantity Perception for Multi‑object Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.10112)


* **[ICLR 2025]** *Lightning‑Fast Image Inversion and Editing for Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=t9l63huPRt) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://barakmam.github.io/rnri.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/dvirsamuel/NewtonRaphsonInversion)


* **[ICLR 2025]** *Multi‑Reward as Condition for Instruction‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=9RFocgIccP) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/bytedance/Multi-Reward-Editing)


* **[ICLR 2025]** ***HQ‑Edit:*** *A High‑Quality Dataset for Instruction‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=mZptYYttFj) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://thefllood.github.io/HQEdit_web/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/UCSC-VLAA/HQ-Edit) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/UCSC-VLAA/HQ-Edit)


* **[ICLR 2025]** ***CLIPDrag:*** *Combining Text‑based and Drag‑based Instructions for Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=2HjRezQ1nj) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/HKUST-LongGroup/CLIPDrag)


* **[ICLR 2025]** *Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=Hu0FSOSEyS) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://rf-inversion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LituRout/RF-Inversion)


* **[ICLR 2025]** ***PostEdit:*** *Posterior Sampling for Efficient Zero‑Shot Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=J8YWCBPgx7) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/TFNTF/PostEdit)


* **[ICLR 2025]** ***OmniEdit:*** *Building Image Editing Generalist Models Through Specialist Supervision*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=Hlm0cga0sv) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://tiger-ai-lab.github.io/OmniEdit/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/TIGER-AI-Lab/OmniEdit) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/TIGER-Lab/OmniEdit-Filtered-1.2M)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>✨ 2024</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2024]** ***InfEdit:*** *Inversion‑Free Image Editing with Natural Language*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.04965.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sled-group.github.io/InfEdit/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/sled-group/InfEdit)


* **[CVPR 2024]** ***CrossSelfAttention:*** *Towards Understanding Cross and Self‑Attention in Stable Diffusion for Text‑Guided Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.03431.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/alibaba/EasyNLP/tree/master/diffusion/FreePromptEditing)


* **[CVPR 2024]** ***DAC:*** *Doubly Abductive Counterfactual Inference for Text‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.02981.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/xuesong39/DAC)


* **[CVPR 2024]** ***FoI:*** *Focus on Your Instruction: Fine‑grained and Multi‑instruction Image Editing by Attention Modulation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.10113.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/guoqincode/Focus-on-Your-Instruction)


* **[CVPR 2024]** ***CDS:*** *Contrastive Denoising Score for Text‑guided Latent Diffusion Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.18608.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://hyelinnam.github.io/CDS/)


* **[CVPR 2024]** ***DragDiffusion:*** *Harnessing Diffusion Models for Interactive Point‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2306.14435.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yujun-shi.github.io/projects/dragdiffusion.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Yujun-Shi/DragDiffusion)


* **[CVPR 2024]** ***DiffEditor:*** *Boosting Accuracy and Flexibility on Diffusion‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.02583.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/MC-E/DragonDiffusion)


* **[CVPR 2024]** ***FreeDrag:*** *Feature Dragging for Reliable Point‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2307.04684.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LPengYang/FreeDrag)


* **[CVPR 2024]** ***Learnable Regions:*** *Text‑Driven Image Editing via Learnable Regions*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.16432.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yuanze-lin.me/LearnableRegions_page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/yuanze-lin/Learnable_Regions)


* **[CVPR 2024]** ***LEDITS++:*** *Limitless Image Editing using Text‑to‑Image Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.16711.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://leditsplusplus-project.static.hf.space/index.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://huggingface.co/spaces/editing-images/leditsplusplus/tree/main) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/editing-images/leditsplusplus)


* **[CVPR 2024]** ***SmartEdit:*** *Exploring Complex Instruction‑based Image Editing with Large Language Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.06739.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yuzhou914.github.io/SmartEdit/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/TencentARC/SmartEdit)


* **[CVPR 2024]** ***Edit One for All:*** *Interactive Batch Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2401.10219.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://thaoshibe.github.io/edit-one-for-all/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/thaoshibe/edit-one-for-all)


* **[CVPR 2024]** ***DiffMorpher:*** *Unleashing the Capability of Diffusion Models for Image Morphing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.07409.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://kevin-thu.github.io/DiffMorpher_page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Kevin-thu/DiffMorpher)


* **[CVPR 2024]** ***TiNO‑Edit:*** *Timestep and Noise Optimization for Robust Diffusion‑Based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.11120.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/SherryXTChen/TiNO-Edit)


* **[CVPR 2024]** ***Person in Place:*** *Generating Associative Skeleton‑Guidance Maps for Human‑Object Interaction Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Person_in_Place_Generating_Associative_Skeleton-Guidance_Maps_for_Human-Object_Interaction_CVPR_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yangchanghee.github.io/Person-in-Place_page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/YangChangHee/CVPR2024_Person-In-Place_RELEASE)


* **[CVPR 2024]** ***Referring Image Editing:*** *Object‑level Image Editing via Referring Expressions*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Referring_Image_Editing_Object-level_Image_Editing_via_Referring_Expressions_CVPR_2024_paper.pdf)


* **[CVPR 2024]** ***Prompt Augmentation:*** *Prompt Augmentation for Self‑supervised Text‑guided Image Manipulation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Bodur_Prompt_Augmentation_for_Self-supervised_Text-guided_Image_Manipulation_CVPR_2024_paper.pdf)


* **[CVPR 2024]** ***StyleFeatureEditor:*** *The Devil is in the Details — StyleFeatureEditor for Detail‑Rich StyleGAN Inversion and High Quality Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Bobkov_The_Devil_is_in_the_Details_StyleFeatureEditor_for_Detail-Rich_StyleGAN_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/AIRI-Institute/StyleFeatureEditor)


* **[ECCV 2024]** ***RegionDrag:*** *Fast Region‑Based Image Editing with Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://arxiv.org/pdf/2407.18247v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://visual-ai.github.io/regiondrag/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Visual-AI/RegionDrag)


* **[ECCV 2024]** ***TurboEdit:*** *Instant Text‑Based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.08332v1.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://betterze.github.io/TurboEdit/)


* **[ECCV 2024]** ***InstructGIE:*** *Towards Generalizable Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.05018.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://cr8br0ze.github.io/InstructGIE)


* **[ECCV 2024]** ***StableDrag:*** *Stable Dragging for Point‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.04437.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://stabledrag.github.io/)


* **[ECCV 2024]** ***Eta Inversion:*** *Designing an Optimal Eta Function for Diffusion‑based Real Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02157.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/furiosa-ai/eta-inversion)


* **[ECCV 2024]** ***SwapAnything:*** *Enabling Arbitrary Object Swapping in Personalized Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04768.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://swap-anything.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/eric-ai-lab/swap-anything)


* **[ECCV 2024]** ***Guide‑and‑Rescale:*** *Self‑Guidance Mechanism for Effective Tuning‑Free Real Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08987.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/AIRI-Institute/Guide-and-Rescale)


* **[ECCV 2024]** ***FreeDiff:*** *Progressive Frequency Truncation for Image Editing with Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00759.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Thermal-Dynamics/FreeDiff)


* **[ECCV 2024]** ***Lazy Diffusion Transformer:*** *Lazy Diffusion Transformer for Interactive Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03436.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://lazydiffusion.github.io/)


* **[ECCV 2024]** ***ByteEdit:*** *Boost, Comply and Accelerate Generative Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00359.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://byte-edit.github.io/)


* **[ICLR 2024]** ***MGIE:*** *Guiding Instruction‑based Image Editing via Multimodal Large Language Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2309.17102.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://mllm-ie.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/apple/ml-mgie)


* **[ICLR 2024]** ***SDE‑Drag:*** *The Blessing of Randomness — SDE Beats ODE in General Diffusion‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.01410.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ml-gsai.github.io/SDE-Drag-demo/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ML-GSAI/SDE-Drag)


* **[ICLR 2024]** ***Motion Guidance:*** *Diffusion‑Based Image Editing with Differentiable Motion Estimators*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2401.18085.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://dangeng.github.io/motion_guidance/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/dangeng/motion_guidance)


* **[ICLR 2024]** ***OIR:*** *Object‑Aware Inversion and Reassembly for Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.12149.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://aim-uofa.github.io/OIR-Diffusion/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/aim-uofa/OIR)


* **[ICLR 2024]** ***Noise Map Guidance:*** *Inversion with Spatial Context for Real Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.04625.pdf)


* **[AAAI 2024]** ***TIC:*** *Tuning‑Free Inversion‑Enhanced Control for Consistent Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.14611.pdf)


* **[AAAI 2024]** ***BARET:*** *Balanced Attention based Real Image Editing driven by Target‑text Inversion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.05482.pdf)


* **[AAAI 2024]** ***CacheEdit:*** *Accelerating Text‑to‑Image Editing via Cache‑Enabled Sparse Diffusion Inference*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.17423.pdf)


* **[AAAI 2024]** ***High‑Fidelity Editing:*** *High‑Fidelity Diffusion‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.15707.pdf)


* **[AAAI 2024]** ***AdapEdit:*** *Spatio‑Temporal Guided Adaptive Editing Algorithm for Text‑Based Continuity‑Sensitive Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.08019.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/AnonymousPony/adap-edit)


* **[AAAI 2024]** ***TexFit:*** *Text‑Driven Fashion Image Editing with Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://ojs.aaai.org/index.php/AAAI/article/view/28885)

</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>✨ 2023</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2023]** ***Diffusion Disentanglement:*** *Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Uncovering_the_Disentanglement_Capability_in_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://wuqiuche.github.io/DiffusionDisentanglement-project-page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement)

* **[CVPR 2023]** ***SINE:*** *SINgle Image Editing with Text-to-Image Diffusion Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_SINE_SINgle_Image_Editing_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zhang-zx.github.io/SINE/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zhang-zx/SINE)

* **[CVPR 2023]** ***Imagic:*** *Text-Based Real Image Editing with Diffusion Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kawar_Imagic_Text-Based_Real_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://imagic-editing.github.io/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/fffiloni/imagic-stable-diffusion)

* **[CVPR 2023]** ***InstructPix2Pix:*** *Learning to Follow Image Editing Instructions*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Brooks_InstructPix2Pix_Learning_To_Follow_Image_Editing_Instructions_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://www.timothybrooks.com/instruct-pix2pix/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/timothybrooks/instruct-pix2pix) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/timbrooks/instruct-pix2pix)

* **[CVPR 2023]** ***Null-text Inversion:*** *Null-text Inversion for Editing Real Images using Guided Diffusion Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Mokady_NULL-Text_Inversion_for_Editing_Real_Images_Using_Guided_Diffusion_Models_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://null-text-inversion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/google/prompt-to-prompt)

* **[ICCV 2023]** ***MasaCtrl:*** *Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_MasaCtrl_Tuning-Free_Mutual_Self-Attention_Control_for_Consistent_Image_Synthesis_and_ICCV_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ljzycmd.github.io/projects/MasaCtrl/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/TencentARC/MasaCtrl)

* **[ICCV 2023]** ***Local Prompt Mixing:*** *Localizing Object-level Shape Variations with Text-to-Image Diffusion Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Patashnik_Localizing_Object-Level_Shape_Variations_with_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://orpatashnik.github.io/local-prompt-mixing/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/orpatashnik/local-prompt-mixing) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/orpatashnik/local-prompt-mixing)

* **[ICLR 2022]** ***SDEdit:*** *Guided Image Synthesis and Editing with Stochastic Differential Equations*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2108.01073.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sde-image-editing.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ermongroup/SDEdit)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

</details>

</details>

[<small>⇧ Back to ToC</small>](#contents)



---

## <span id="datasets">🗂️ Datasets</span>
| Dataset Name | Year | Modalities | Task | Paper | Link |
| :--- | :--- | :--- | :--- | :---: | :---: |
| **MS COCO** | 2014 | Text, Image | Text-to-Image Generation, Image Captioning | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/1405.0312) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://cocodataset.org/#home) |
| **Oxford-120 Flowers**| 2008 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://www.robots.ox.ac.uk/~vgg/publications/2008/Nilsback08/nilsback08.pdf) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/) |
| **CUB-200-2011** | 2011 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://resolver.caltech.edu/CaltechCSTR:2010.001) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](http://www.vision.caltech.edu/datasets/cub_200_2011/) |
| **LAION-5B** | 2022 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2210.08402) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://laion.ai/blog/laion-5b/) |
| **DiffusionDB** | 2022 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2210.14896) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://poloclub.github.io/diffusiondb/) |

[<small>⇧ Back to ToC</small>](#contents)

---

## <span id="about-us">🎓 About Us</span>

QuenithAI is a professional organization composed of top researchers, dedicated to providing high-quality 1-on-1 research mentoring for university students worldwide. Our mission is to help students bridge the gap from theoretical knowledge to cutting-edge research and publish their work in top-tier conferences and journals.

Maintaining this `Awesome Text-to-Image Generation` list requires significant effort, just as completing a high-quality paper requires focused dedication and expert guidance. If you're looking for one-on-one support from top scholars on your own research project, to quickly identify innovative ideas and make publications, we invite you to contact us ASAP.

➡️ **Contact us via [WeChat](assets/wechat.jpg) or [E-mail](mailto:your.email@example.com) to start your research journey.**

---

「应达学术」(QuenithAI) 是一家由顶尖研究者组成，致力于为全球高校学生提供高质量1V1科研辅导的专业机构。我们的使命是帮助学生培养出色卓越的科研技能，在顶级会议和期刊上发表自己的成果。

维护一个GitHub调研仓库需要巨大的精力，正如完成一篇高质量的论文一样，离不开专注的投入和专业的指导。如果您希望在自己的研究项目中，获得来自顶尖学者的一对一支持，我们诚邀您与我们取得联系。

➡️ **欢迎通过 [微信](assets/wechat.jpg) 或 [邮件](mailto:your.email@example.com) 联系我们，开启您的科研之旅。**


[<small>⇧ Back to ToC</small>](#contents)

---



## <span id="contributing">🤝 Contributing</span>

Contributions are welcome! Please see our [**Contribution Guidelines**](CONTRIBUTING.md) for details on how to add new papers, correct information, or improve the repository.
